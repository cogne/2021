{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to the first-ever (and hopefully last-ever) remote ADA final exam (Fall 2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exam, you will analyze data from Wikipedia and from the [Wikispeedia](https://dlab.epfl.ch/wikispeedia/play/) game, which you have already encountered in lecture 12 on [“Handling networks”](https://docs.google.com/presentation/d/1h6cIINJ9cNZ-Rtb7SskXrl9Xet5zPUoX2oJlNM0loHQ/edit#slide=id.g464f30ace1_0_92). The rules of the Wikispeedia game are simple:\n",
    "1. Players are given two Wikipedia articles: a *source* and a *target*.\n",
    "2. Starting from the source article, the goal is to reach the target, exclusively by following links in the articles encountered along the path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data\n",
    "\n",
    "The Wikispeedia data comprises ~76K games by human players, thereby capturing human navigation behavior on a very small subset of English Wikipedia comprising ~4.5K articles with a total of ~120K links between them.\n",
    "\n",
    "The dataset is available in the `data` directory pushed to the same GitHub repo as the exam. Inside the data directory, you will find four files:\n",
    "\n",
    "####  1. `article_df_task-A.tsv.gz`: A tab-separated file with the following information about Wikipedia articles\n",
    "- *name*: The name of the Wikipedia article\n",
    "- *topic*: The top-level topic label for the article (each article is assigned exactly one topic label)\n",
    "- *content*: The cleaned text of the article (having removed punctuations, stopwords, numbers, and letter casing). It is stored as a space-separated string.\n",
    "\n",
    "#### 2. `links_task-B.tsv.gz`: A tab-separated file containing Wikipedia links\n",
    "- *linkSource*: The name of the source Wikipedia article from which the link originates\n",
    "- *linkTarget*: The name of the target Wikipedia article to which the link points\n",
    "\n",
    "#### 3.  `paths_df_task-B.tsv.gz`: A tab-separated file with the following information about the played games\n",
    "- *hashed_ip_address*: Anonymized IP address of the player\n",
    "- *source*: The name of the Wikipedia article from where the game started\n",
    "- *target*: The name of the target article that the player was supposed to reach \n",
    "- *finished*: Whether the game was successfully finished (whether the player reached the target article)\n",
    "- *human_path_length*: The number of clicks made by the player, before they either reached the target (in *finished* games) or gave up (in *unfinished* games)\n",
    "- *shortest_path_length*: The minimum number of clicks required to reach the target article from the source article. Disconnected source-target article pairs have a shortest_path_length of *-1*.\n",
    "- *duration_in_sec*: The duration of the game in seconds\n",
    "\n",
    "#### 4.  `paths_df_task-C.tsv.gz`: In continuation to the task-B paths dataframe, this tab-separated file has the following additional information\n",
    "- *in_degree_target*: In-degree of the target article, that is the number of incoming links that lead to the target article\n",
    "\n",
    "*Note: In all the aforementioned files, the first line contains the header information. Additionally, article names are URL-encoded. For example, 'Antonín_Dvořák' is stored as 'Anton%C3%ADn_Dvo%C5%99%C3%A1k'. If needed, you can decode the article names using this code: `from urllib.parse import unquote; unquote('article-name')`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Some minimalistic imports '''\n",
    "import pandas as pd\n",
    "import random\n",
    "import gzip\n",
    "import operator\n",
    "import time\n",
    "from urllib.parse import unquote\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, average_precision_score, balanced_accuracy_score\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "from IPython.display import Image\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task A. Can we predict article topic using textual content?\n",
    "\n",
    "*Note-1: For the entire Task A, we will use logistic regression for its scalability and simplicity. Specifically, we will use [SGDClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) available in `sklearn` with 'log' loss and '5' epochs for training a logistic regression classifier.*\n",
    "\n",
    "*Note-2: In all the cases where random number generation is required (train-test split, fitting the model, etc.), make sure to use 42 as the random seed.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A1. Load the text data and analyze the topic distribution\n",
    "1. Load `data/article_df_task-A.tsv.gz` as a pandas dataframe.   \n",
    "   *Note: we have already cleaned the text for you!*\n",
    "2. Print the total number of unique topics. Additionally, using a plot of appropriate type, analyze the article topic distribution.   \n",
    "   **/Discuss:/** What do you observe? Are different topics equally likely? If not, is there a large disparity between certain highly popular and very rare topics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''your code here''';\n",
    "articles_content = pd.read_csv('data/article_df_task-A.tsv.gz', sep=\"\\t\", compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dist = articles_content['topic'].value_counts()\n",
    "print(\"Total number of unique topics are: \", len(topic_dist))\n",
    "topic_dist.plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answering A1.2:**   \n",
    "There is a substantial disparity in topic frequency of different topics. Certain topics such as 'Science' and 'Geography' are highly frequent, while others such as 'Mathematics' and 'Art' are super rare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2. Topic classification (multi-class): Articles from 5 most frequent topics\n",
    "\n",
    "#### A2.1 Prepping the data for ML\n",
    "1. Restrict the data to articles corresponding to the top-5 topics based on topic frequency.\n",
    "2. Construct a TF-IDF representation of textual content.\n",
    "3. Classes are defined by the topic labels, which are given as *strings*. However, for training classifiers via `sklearn` the class labels need to be encoded as ordinals. Perform this encoding and add a new column of ordinal class labels in the current dataframe.\n",
    "4. Create two numpy arrays: X (TF-IDF representation of each article) and y (class-label of each article).\n",
    "\n",
    "#### A2.2 Train and evaluate a multi-class classifier\n",
    "1. Perform a train-test split with 70% as the training portion and the remainder as the testing portion. Use `random_state=42` for fixing the seed to 42.\n",
    "2. Train a logistic regression classifier with L2 regularization using the `SGDClassifier()` method of `sklearn`. Use `SGDClassifier(loss='log', max_iter=5, tol=None, alpha=1e-4, random_state=42)` to initialize the classifier object. Do not modify the settings for any other parameter, let them be set to their default values.   \n",
    "   **Important heads-up**: *Depending on the configuration of your computer, training the model could take up to 1 minute of compute time.*\n",
    "3. Report the classifier performance on the test data using accuracy as the metric.\n",
    "4. **/Discuss:/** Are you satisfied with the model performance? Qualitatively discuss the performance of the model by comparing it to a random baseline and justify your response.\n",
    "\n",
    "**Important note:** *Based on the discussion in the class, logistic regression is a binary classification technique. A simple heuristic to perform multi-class classification using a binary classifier is to use the [one-vs.-rest (OVR)](https://en.wikipedia.org/wiki/Multiclass_classification#One-vs.-rest) strategy. In this strategy, a binary classifier is trained for each class with the capability to differentiate it from all the other classes, and thus, $k$ binary classifiers are trained for a $k$-class classification. Internally, `SGDClassifier()` uses the [OVR](https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html) strategy of `sklearn` whenever it sees the number of unique values in the **y** vector to be greater than 2. Thus, you can simply use it as a black-box! The only important point for this part is to make sure that the **y** vector of class-labels should have **5** unique values, one corresponding to each class-label.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''your answer/code here''';\n",
    "top5 = list(topic_dist.index)[0:5]\n",
    "top5_articles_content = articles_content.loc[articles_content['topic'].apply(lambda x: x in top5)].copy()\n",
    "'''\n",
    "another way\n",
    "top5_articles_content = articles_content.loc[articles_content['topic'].isin(top5)].copy()\n",
    "'''\n",
    "factor = pd.factorize(top5_articles_content['topic'])\n",
    "top5_articles_content['labels'] = factor[0]\n",
    "top5_definitions = factor[1]\n",
    "print(top5_definitions)\n",
    "top5_articles_content.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = top5_articles_content['content'].to_numpy()\n",
    "y = top5_articles_content['labels'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "parameters = {\n",
    "    'clf__alpha': [1e-4],    \n",
    "}\n",
    "\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier(penalty='l2', loss='log', max_iter=5, tol=None, random_state=42))\n",
    "])\n",
    "\n",
    "tstart = time.time()\n",
    "gs_clf = GridSearchCV(text_clf, parameters, cv=5)\n",
    "gs_clf = gs_clf.fit(X_train, y_train)\n",
    "tend = time.time()\n",
    "print(tend-tstart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gs_clf.best_score_)\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = gs_clf.predict(X_test)\n",
    "print(\"Accuracy on Test Data: \", np.mean(predicted == y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answering A2.2.4:**   \n",
    "In a 5-class classification a random model obtains an accuracy of 20% in expectation. Thus, obtaining an accuracy in high 80s is a strong outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A3. Topic classification (binary): Articles from the most and least frequent topics\n",
    "\n",
    "#### A3.1 Train and evaluate a binary classifier\n",
    "1. Restrict the data to articles belonging to the most frequent or to the least frequent topic.\n",
    "2. Create two numpy arrays: X (TF-IDF representation of each article) and y (class-label of each article).\n",
    "3. Perform a train-test split with 70% as the training portion and the remainder as the testing portion. Use `random_state=42` for fixing the seed to 42.\n",
    "4. Train a logistic regression classifier with L2 regularization using the `SGDClassifier()` method of `sklearn`. Use `SGDClassifier(loss='log', max_iter=5, tol=None, alpha=1e-4, random_state=42)` to initialize the classifier object. Do not modify the settings for any other parameter, let them be set to their default values.\n",
    "5. Report the classifier performance on the test data using accuracy as the metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''your answer/code here''';\n",
    "top_and_bottom = list(topic_dist.index)[::len(list(topic_dist.index))-1]\n",
    "top_and_bottom_articles_content = articles_content.loc[articles_content['topic'].apply(lambda x: x in top_and_bottom)].copy()\n",
    "'''\n",
    "another way\n",
    "top_and_bottom_articles_content = articles_content.loc[articles_content['topic'].isin(top_and_bottom)].copy()\n",
    "'''\n",
    "factor = pd.factorize(top_and_bottom_articles_content['topic'])\n",
    "top_and_bottom_articles_content['labels'] = factor[0]\n",
    "top_and_bottom_definitions = factor[1]\n",
    "print(top_and_bottom_definitions)\n",
    "top_and_bottom_articles_content.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_small = top_and_bottom_articles_content['content'].to_numpy()\n",
    "y_small = top_and_bottom_articles_content['labels'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_small, X_test_small, y_train_small, y_test_small = train_test_split(X_small, y_small, test_size=0.3, random_state=42)\n",
    "\n",
    "parameters = {\n",
    "    'clf__alpha': [1e-4],\n",
    "}\n",
    "\n",
    "text_clf_small = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier(penalty='l2', loss='log', max_iter=5, tol=None, random_state=42))\n",
    "])\n",
    "\n",
    "tstart = time.time()\n",
    "gs_clf_small = GridSearchCV(text_clf_small, parameters, cv=5)\n",
    "gs_clf_small = gs_clf_small.fit(X_train_small, y_train_small)\n",
    "tend = time.time()\n",
    "print(tend-tstart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gs_clf_small.best_score_)\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf_small.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_small = gs_clf_small.predict(X_test_small)\n",
    "print(\"Accuracy on Test Data: \", np.mean(predicted_small == y_test_small))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A3.2 Assessing the evaluation metric\n",
    "1. **/Discuss:/** What is your take on the accuracy obtained in A3.1? Do you think accuracy is the correct evaluation metric for this task? If yes, justify! If not, why not, and what else can be used?\n",
    "2. If you included additional evaluation metrics in A3.2.1, then perform an empirical analysis of the performance using these metrics.   \n",
    "   **/Discuss:/** Additionally, discuss what you observe. Is it harder to predict the topic labels of certain articles? Why, or why not? Please elaborate!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answering A3.2.1**   \n",
    "There is a huge class imbalance, thus, accuracy is not a good metric. It is very easy for accuracy to be biased. If you label everything as the majority class, the accuracy would still be very high (in the high 90s), but the performance on the minority class would be terrible.\n",
    "\n",
    "In such cases, one can use the either of the following for evaluation.\n",
    "1. confusion matrix, \n",
    "2. balanced accuracy score (sklearn), or \n",
    "3. (un)weighted micro/macro averaged F1 scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answering A3.2.2**   \n",
    "It is hard to correctly predict the topic labels of the articles in the minority class. This could be because of the following reasons:\n",
    "1. The model didn't see enough training data from the minority class to learn to classify/discriminate it.\n",
    "2. The loss function treats each training sample as equally important. Thus, trying to minimize the overall loss would  guide the model to focus more on the majority class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### At least one of the following should be be used to report the performance. If a reasonable alternate metric is suggested, judge based on it's correctness!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test_small, predicted_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(balanced_accuracy_score(y_test_small, predicted_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_small, predicted_small, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A3.3 Class proportions\n",
    "1. **/Discuss:/** Does the disparity in class proportions hurt the model? If yes, how can you fix it? If not, justify the reasons behind your choice.   \n",
    "   *Hint: The learning objective of a classifier can be modified by altering the importance of each class in the computation of the loss function.*\n",
    "2. If your answer to the aforementioned question is a \"yes\", please do the following. If not, move to **Task A4**.\n",
    "    * Implement the fix you proposed in A3.3.1, and repeat the classification task performed in Step A3.1 using the fixed model.\n",
    "    * Vary the regularization parameter `alpha` in the range [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1], and using accuracy as the metric find and report its optimal value using 5-fold cross validation on the training set.   \n",
    "       **Important heads-up**: *Depending on the configuration of your computer, grid search for the optimal value of `alpha` could take up to 1 minute of compute time.*\n",
    "    * Lastly, redo the evaluations proposed in A3.2.   \n",
    "       **/Discuss:/** Do you observe any differences from the results obtained in A3.2? Why, or why not? Please elaborate!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answering A3.3.1**   \n",
    "As evident from the confusion matrix, the disparity in class proportions does indeed hurt the model. Almost all (99%) of the examples are classified as class 0. What's more, 75% of the articles in class 1, are predicted to belong to class 0.\n",
    "\n",
    "One way to address this is to employ cost-sensitive learning, i.e., using `class-weight`='balanced' while training the model. Another way could be up/down sampling minority/majority class. 'SMOTE' is a very popular technique of oversampling the minority class that can be employed here.\n",
    "\n",
    "The solution described below uses `class-weight`='balanced', however, if someone uses 'SMOTE' that's acceptable as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A3.3.2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'clf__alpha': [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6],\n",
    "}\n",
    "\n",
    "text_clf_small = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier(penalty='l2', loss='log', max_iter=5, tol=None, random_state=42, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "tstart = time.time()\n",
    "gs_clf_small = GridSearchCV(text_clf_small, parameters, cv=5)\n",
    "gs_clf_small = gs_clf_small.fit(X_train_small, y_train_small)\n",
    "tend = time.time()\n",
    "print(tend-tstart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gs_clf_small.best_score_)\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"The optimal %s is: %r\" % (param_name, gs_clf_small.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_small = gs_clf_small.predict(X_test_small)\n",
    "print(\"Accuracy on Test Data: \", np.mean(predicted_small == y_test_small))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similar to A3.2, at least one of the following should be be used to report the performance. If a reasonable alternate metric is suggested, judge based on it's correctness!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test_small, predicted_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(balanced_accuracy_score(y_test_small, predicted_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test_small, predicted_small, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ansering A3.3.2**   \n",
    "The results are much better now as both the minority and majoirty classes are properly classified. This is evident from the confusion matrix, balanced accuracy score, or macro averaged f1-score.\n",
    "\n",
    "Using 'balanced' class weights while training the model forces the loss function to give higher relative importance to training samples corresponding to the minority class. Actually, the training samples are weighted as inverse of the class proportions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A4. Revisiting Multi-class classifier of A2.2\n",
    "\n",
    "1. Print and analyze the confusion matrix for the multi-class model trained in A2.2. **/Discuss:/** What do you observe?\n",
    "2. **/Discuss:/** Similar to A3.3, does the disparity in class proportions hurt the model? If yes, how can you fix it? If not, justify the reasons behind your choice.\n",
    "3. If your answer to the aforementioned question is a \"yes\", please do the following. If not, move to **Task A5**.\n",
    "    * Implement the fix you proposed in A4.2, and repeat the classification task performed in Step A2.2 using the fixed model.\n",
    "    * Vary the regularization parameter `alpha` in the range [1e-6, 1e-5, 1e-4, 1e-3], and using accuracy as the metric find and report its optimal value using 5-fold cross validation on the training set.   \n",
    "       **Important heads-up**: *Depending on the configuration of your computer, grid search for the optimal value of `alpha` could take up to 2 minutes of compute time.*\n",
    "    * Lastly, obtain the accuracy and confusion matrix.\n",
    "4. **/Discuss:/** Contrast the confusion matrix obtained in A4.1 with the one obtained in A4.3. Do you observe any differences? Why, or why not? Please elaborate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''your answer/code here''';\n",
    "print(confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following two metrics are not required/mandatory per the question!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(balanced_accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, predicted, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answering A4.1**   \n",
    "The two least frequent classes have a much higher error than the other three more frequent classes. In fact, the error is somewhat inversely correlated with the frequency of the class. The higher the frequency the lesser the error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answering A4.2**   \n",
    "As evident from the confusion matrix, the disparity in class proportions does indeed hurt the model.\n",
    "\n",
    "One way to address this is to employ cost-sensitive learning, i.e., using `class-weight`='balanced' while training the model. Another way could be up/down sampling minority/majority class. 'SMOTE' is a very popular technique of oversampling the minority class that can be employed here.\n",
    "\n",
    "The solution described below uses `class-weight`='balanced', however, if someone uses 'SMOTE' that's acceptable as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'clf__alpha': [1e-3, 1e-4, 1e-5, 1e-6],\n",
    "}\n",
    "\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier(penalty='l2', loss='log', max_iter=5, tol=None, random_state=42, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "tstart = time.time()\n",
    "gs_clf = GridSearchCV(text_clf, parameters, cv=5)\n",
    "gs_clf = gs_clf.fit(X_train, y_train)\n",
    "tend = time.time()\n",
    "print(tend-tstart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gs_clf.best_score_)\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"The optimal %s is: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = gs_clf.predict(X_test)\n",
    "print(\"Accuracy on Test Data: \", np.mean(predicted == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following two metrics are not required/mandatory per the question!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(balanced_accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, predicted, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answering A4.4**   \n",
    "The results are much better now: the two low frequency classes are now better classified. This is evident from the confusion matrix, balanced accuracy score, or macro averaged f1-score.\n",
    "\n",
    "Using 'balanced' class weights while training the model forces the loss function to give higher relative importance to training samples corresponding to the classes with lower frequency. Actually, the training samples are weighted as inverse of the class proportions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A5. Interpretability\n",
    "1. For each of the 5-classes considered in **A4**, obtain a list of top-10 words (sorted in descending order of their importance) based on the feature weights learned by the classifier.\n",
    "\n",
    "2. **/Discuss:/** Qualitatively discuss the relevance of the identified words and their association with the respective class-labels. Is the classifier correct to assign them high weights? Are they truly discriminative/predictive of the respective class-labels? Justify your responses with clear and crisp reasons!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''your answer/code here''';\n",
    "top5_coeff_indices = np.argsort(gs_clf.best_estimator_.named_steps['clf'].coef_)[:,-10:][:,::-1]\n",
    "print(top5_definitions)\n",
    "np.array(gs_clf.best_estimator_.named_steps['vect'].get_feature_names())[top5_coeff_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answering A5.2**   \n",
    "It is quite clear that the top-10 words are discriminative/predictive of the corresponding class-labels.   \n",
    "For example, 'life', 'death', 'work', 'born', 'career' are quite highly associated with the 'people' class, which is intuitive.   \n",
    "Similarly, 'roman', 'war', 'empire', 'century' being associated with 'history' class is also quite intuitive.   \n",
    "Also, similar points can be said about the other classes as well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task B. The Wikipedia link network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B1. Wikipedia network: reveal your shape!\n",
    "#### B1.1 Load the Wikipedia link network (`data/links_task-B.tsv.gz`) into memory as a directed graph using `NetworkX`.\n",
    "1. Compute the following from the loaded graph object:\n",
    "    * the number of nodes,\n",
    "    * the number of edges, and\n",
    "    * the average degree.\n",
    "2. **/Discuss:/** In order to summarize the degree distribution in a single number, would you recommend using the average degree? Why, or why not? If not, what alternatives can you think of? Please elaborate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''your answer/code here''';\n",
    "flinks = gzip.open(\"data/links_task-B.tsv.gz\")\n",
    "edgeList = []; nodes = {}; edges = {}\n",
    "idx = 0\n",
    "for line in flinks:\n",
    "    line = line.decode('utf-8').strip()\n",
    "    if idx==0 or '#' in line or len(line)==0:\n",
    "        idx+=1\n",
    "        continue\n",
    "    line = line.split(\"\\t\")\n",
    "    source = line[0]; target = line[1]\n",
    "    nodes[source] = True; nodes[target] = True\n",
    "    edges[(source,target)] = True\n",
    "    edgeList.append((source,target))\n",
    "    idx+=1\n",
    "    \n",
    "G = nx.DiGraph()\n",
    "G.add_edges_from(edgeList)\n",
    "print(len(G.nodes()), len(G.edges()), len(G.edges())/len(G.nodes()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answering B1.1.2**   \n",
    "Average degree is not recommended as the degree distribution of real-world networks usually follows a powerlaw. Summarizing powerlaws with average values is not a good idea, as there is a long tail, and there are many nodes that have very high degree. Instead, median is a better choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B1.2 Using the appropriate plot type, plot the in-degree and out-degree distributions of the Wikipedia link network and analyze it. \n",
    "1. **/Discuss:/** What is the appropriate scale of the axes?\n",
    "2. **/Discuss:/** Does the distribution (roughly) follow a particular distribution, and if yes, which one? Additionally, explain how you arrived at your conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''your answer/code here''';\n",
    "def degree_histogram_directed(G, in_degree=False, out_degree=False):\n",
    "    nodes = G.nodes()\n",
    "    if in_degree:\n",
    "        in_degree = dict(G.in_degree())\n",
    "        degseq=[in_degree.get(k,0) for k in nodes]\n",
    "    elif out_degree:\n",
    "        out_degree = dict(G.out_degree())\n",
    "        degseq=[out_degree.get(k,0) for k in nodes]\n",
    "    else:\n",
    "        degseq=[v for k, v in G.degree()]\n",
    "    dmax=max(degseq)+1\n",
    "    freq= [ 0 for d in range(dmax) ]\n",
    "    for d in degseq:\n",
    "        freq[d] += 1\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_degree_freq = degree_histogram_directed(G, in_degree=True)\n",
    "out_degree_freq = degree_histogram_directed(G, out_degree=True)\n",
    "degrees = range(len(in_degree_freq))\n",
    "fig, axes = plt.subplots(2, 2, figsize = (15,10),gridspec_kw={'hspace': 0.4, 'wspace': 0.2})\n",
    "fig.suptitle(\"In and out degree distribution of the Wikispeedia Network (Left: linear axes, Right: Log axes)\", fontsize=20)\n",
    "axes[0][0].plot(range(len(in_degree_freq)), in_degree_freq, 'go-', label='In-degree')\n",
    "axes[0][1].loglog(range(len(in_degree_freq)), in_degree_freq, 'go-', label='In-degree') \n",
    "axes[1][0].plot(range(len(out_degree_freq)), out_degree_freq, 'bo-', label='Out-degree')\n",
    "axes[1][1].loglog(range(len(out_degree_freq)), out_degree_freq, 'bo-', label='Out-degree')\n",
    "for x in range(0,axes.shape[0]):\n",
    "    for y in range(0,axes.shape[1]):\n",
    "        axes[x,y].set_xlabel('Degree', fontsize = 20)\n",
    "        axes[x,y].set_ylabel('Frequency', fontsize = 20)\n",
    "        axes[x,y].legend(fontsize=15)\n",
    "fig.subplots_adjust(top=0.94)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B1.2.1** Appropriate scale is log-log.   \n",
    "**B1.2.2** distribution follows powerlaw, linear in the log-log plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B1.3 Connectedness of the Wikipedia link network\n",
    "\n",
    "**Definition-1:** An undirected graph $G$ is said to be connected if, for every pair of vertices $(u, v)$, it contains a path that connects the two vertices.\n",
    "\n",
    "**Definition-2:** A directed graph $G_{dir}$ is **weakly connected** (or simply connected) if the underlying undirected graph obtained by replacing all directed edges of the graph with undirected edges is a connected graph (cf. Definition-1). \n",
    "\n",
    "**Definition-3:** A directed graph $G_{dir}$ is **strongly connected** if, for every pair of vertices $(u, v)$, it contains a directed path from $u$ to $v$ and a directed path from $v$ to $u$.\n",
    "\n",
    "Given these definitions, what can you say about the connectedness of the Wikipedia link network? Specifically, answer the following:\n",
    "1. Is the Wikipedia link network weakly connected? If not, print the number of weakly connected components. Additionally, print the number of nodes and edges of the subgraph corresponding to the largest weakly connected component.\n",
    "2. Is the Wikipedia link network strongly connected? If not, print the number of strongly connected components. Additionally, print the number of nodes and edges of the subgraph corresponding to the largest strongly connected component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''your answer/code here''';\n",
    "print(\"Weakly connected: \", nx.is_weakly_connected(G))\n",
    "print(f\"There are {len(list(nx.weakly_connected_components(G)))} weakly connected components\")\n",
    "print(\"Strongly connected: \", nx.is_strongly_connected(G))\n",
    "print(f\"There are {len(list(nx.strongly_connected_components(G)))} strongly connected components\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest_cc = max(nx.weakly_connected_components(G), key=len)\n",
    "H = G.subgraph(list(largest_cc))\n",
    "print(len(H.nodes()), len(H.edges()), len(H.edges())/len(H.nodes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest_scc = max(nx.strongly_connected_components(G), key=len)\n",
    "H = G.subgraph(list(largest_scc))\n",
    "print(len(H.nodes()), len(H.edges()), len(H.edges())/len(H.nodes()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B2. The interplay between human navigation behavior and Wikipedia network structure\n",
    "\n",
    "#### B2.1 Loading and analyzing the game data (3 points)\n",
    "1. Load `data/paths_df_task-B.tsv.gz` as a pandas dataframe.\n",
    "2. Some games were successfully finished (*finished=True*) while others were unsuccessful (i.e., the player gave up before reaching the target article). However, not all unsuccessful games are infeasible; the player might simply not have been good enough at the game to reach the target from the source. **/Discuss:/** Clearly state the reasons why there may be games that are *truly infeasible*.   \n",
    "3. List all unsuccessful games that are *truly infeasible* and remove them from the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answering B2.1.2**   \n",
    "It's impossible to reach a target for games where there is no path existent between the source and the target in the underlying graph, indicated by the shortest_path_length of -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''your answer/code here''';\n",
    "paths = pd.read_csv('data/paths_df_task-B.tsv.gz', sep=\"\\t\", compression='gzip')\n",
    "paths[paths['shortest_path_length'] == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = paths.loc[paths['shortest_path_length'] != -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B2.2 Wikispeedia => Wikislowia?\n",
    "1. Compute and report the average and median path lengths for games that were successfully finished (*finished=True*). You should compute and report path length in two ways: (1) with respect to the number of clicks needed by *human players*, (2) with respect to the *minimum* number clicks an *optimal player* would need.   \n",
    "   **/Discuss:/** What can you say about the navigability of the Wikipedia link network?   \n",
    "   *Hint: Think about [Milgram's experiment](https://docs.google.com/presentation/d/1h6cIINJ9cNZ-Rtb7SskXrl9Xet5zPUoX2oJlNM0loHQ/edit#slide=id.g464f30ace1_0_261) and relate your findings with his findings!*\n",
    "2. **/Discuss:/** Suppose you want to maliciously decrease Wikipedia's navigability. You are allowed to remove certain edges in the network. What criterion would you use to decide which edges should be removed? Additionally, discuss why these edges are critical for Wikipedia's navigability. Justify your answer with clear and substantive reasoning!   \n",
    "   _Hint: Navigability of a network depends on the existence of short paths in the network._\n",
    "3. **/Discuss:/** In continuation to B2.2.2, assume you are given a budget of $k<<n$ edges to be removed. Design the algorithm that you would employ, provide its pseudocode (as a list of steps), and justify your algorithm design choice.   \n",
    "   *Note: You don't have to describe the steps for computing the criterion you identified in Step B.2.2. Instead, using the identified criterion as a blackbox, you have to provide the steps required to remove $k>1$ edges.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''your answer/code here''';\n",
    "paths[paths['finished']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answering B2.2**\n",
    "1. The average and median path length based on human navigation in the completed games is 6.75 and 6, respectively. Similar to the 6-degrees of separation results in Milgram's experiment! Also, avg and median shortest path lengths are 2.85 and 3, respectively, which shows that short paths do exist in the network, and the former result shows that they are discoverable by humans without knowledge of the overall network structure.\n",
    "2. Find the edge with the highest edge betweenness centrality value. Removing such an edge would impact a lot of shortest paths, as the highest fraction of shortest paths pass through this edge.\n",
    "3. Pseudocode: (Iterative greedy)\n",
    "   Step-1: Find the edge with the highest betweenness centrality value.\n",
    "   Step-2: Remove the edge.\n",
    "   Step-3: Repeat Steps 1 and 2, until k edges have been removed.\n",
    "   Note: Removing k-edges based on their betweenness centrality in one-go is not the best way to proceed, iterative greedy is better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B2.3 Relationship between network structure and game completion\n",
    "1. Enrich the dataframe with the in-degree and eigenvector centrality of the target article for each game path.\n",
    "2. Using plots of appropriate type, plot the in-degree distribution of the target articles of game paths. What is the appropriate scale of the axes? Additionally, summarize the in-degree distribution of the target articles in a single number using a suitable metric. Justify your choice of the metric.\n",
    "3. Using plots of appropriate type, analyze the differences in in-degree and eigenvector centrality of finished and unfinished games.   \n",
    "   **/Discuss:/** What do you observe? Clearly discuss your observations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''your answer/code here''';\n",
    "tstart = time.time()\n",
    "eigenvector_centrality = pd.Series(dict(nx.algorithms.eigenvector_centrality(G)))\n",
    "in_degree = pd.Series(dict(G.in_degree()))\n",
    "df_cent = pd.DataFrame([eigenvector_centrality, in_degree]).T.rename({0: \"eigenvector_centrality\",\n",
    "                                                                      1: \"in_degree\"}, axis=1)\n",
    "\n",
    "paths[\"eigenvector_centrality_target\"] = \\\n",
    "    paths.target.apply(lambda x: df_cent[\"eigenvector_centrality\"][x] \n",
    "                      if x in df_cent[\"eigenvector_centrality\"] else None)\n",
    "\n",
    "paths[\"in_degree_target\"] = \\\n",
    "    paths.target.apply(lambda x: df_cent[\"in_degree\"][x] \n",
    "                      if x in df_cent[\"in_degree\"] else None)\n",
    "\n",
    "paths = paths.loc[~paths.in_degree_target.isna()]\n",
    "tend = time.time()\n",
    "print(tend - tstart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B2.3.2: Please note axes labels and titles should be added in the plots presented by the students. Also, we just truncated to max degree of 100 for ease of exposition, such thing is not expected from the students, and if done, should lead to reduction of the points. Also, plots similar to what I made in B1.2 are acceptable (in fact better), and even bar-charts are accepted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(paths[\"in_degree_target\"], range = [0,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(paths[\"in_degree_target\"].mean(), paths[\"in_degree_target\"].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B2.3.3: Even in this scenario bar-charts are acceptable. More fundamentally, any plot that portrays the differences b/w finished and unfinished games for the said metrics (in-degree and eigenvector centrality) is acceptable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "fig, axs = plt.subplots(1,2, figsize=(15, 5))\n",
    "sns.pointplot(x=\"finished\", y=\"eigenvector_centrality_target\", data=paths, ax=axs[0])\n",
    "sns.pointplot(x=\"finished\", y=\"in_degree_target\", data=paths, ax=axs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answering B2.3.3:**   \n",
    "The average eigenvector centrality and in-degree of the end nodes is higher for games that were successfully finished when compared to those that weren't. This is an important signal, as it indicates that nodes with higher in-degree (resp. eigenvector centrality) are easier and better targets for human navigation (could result in shorter paths and faster game completion), which is quite intuitive.\n",
    "It also hints towards the possibility of finding a causal effect, however, further exploration is needed to substantitate that assumption!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task C. Putting on the causality hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C1. Are articles with high in-degree easier to find? A naive analysis\n",
    "\n",
    "Recall that in each Wikispeedia game, a player is **randomly** assigned a source article and a target article. In task **C**, we are interested in measuring the effect of the target article's in-degree on the player's chances of successfully finishing the game.\n",
    "\n",
    "1. Load `data/paths_df_task-C.tsv.gz` as a pandas dataframe.\n",
    "2. Consider target articles with fewer than 20 incoming links to have a *low in-degree* and articles with at least 20 to have a *high in-degree*.\n",
    "3. Calculate the fraction of games with a high-in-degree target that was finished, and the fraction of games with a low-in-degree target that was finished. Do you observe significant differences, and in what direction? (You may do a statistical test or provide confidence intervals.)\n",
    "4. Repeat the same analysis for the game duration, for finished games only.\n",
    "5. Quantify and discuss your findings from Steps C1.3 and C1.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''your answer/code here''';\n",
    "paths = pd.read_csv('data/paths_df_task-C.tsv.gz', sep=\"\\t\", compression='gzip')\n",
    "paths[\"in_degree_binary_target\"] = paths[\"in_degree_target\"] >= 20\n",
    "\n",
    "pfinish_high = (sum(paths.loc[paths[\"in_degree_binary_target\"]].finished)/len(paths.loc[paths[\"in_degree_binary_target\"]]))\n",
    "pfinish_low = (sum(paths.loc[~paths[\"in_degree_binary_target\"]].finished)/len(paths.loc[~paths[\"in_degree_binary_target\"]]))\n",
    "\n",
    "print(pfinish_high)\n",
    "print(pfinish_low)\n",
    "print(pfinish_high - pfinish_low)\n",
    "\n",
    "time_high = paths.loc[(paths[\"in_degree_binary_target\"])&paths['finished']].duration_in_sec.mean()\n",
    "time_low = paths.loc[(~paths[\"in_degree_binary_target\"])&paths['finished']].duration_in_sec.mean()\n",
    "\n",
    "print(time_high)\n",
    "print(time_low)\n",
    "print(time_high - time_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C1: Note that a statistical significance test (any meaningful test or bootstrapped CIs) is expected from students"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answering C1:**   \n",
    "Games with high in-degree targets are 21.61% more *likely to be finished*, and last 69.14 *seconds shorter* if finished."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C2. Modeling the study setup with a causal diagram\n",
    "\n",
    "**C2.1** We are interested in the effect of target in-degree (**deg(T)**) on the binary outcome (**O**) that captures whether the game was successfully finished or not. In the previously conducted naive analysis, we did not take into account the shortest path length (**L**), which is the minimum number of clicks necessary in order to reach the target article from the source article (**S**). Target and source articles are randomly assigned, independently from each other. The assigned source article S and deg(T) affect the shortest path length L. The source article affects the outcome through the shortest path length, and also directly, as some sources might be more central in the network. Choose a causal diagram **(A, B, C, D, E, or F)** that corresponds to the described relationships between the 4 variables:\n",
    "- **deg(T):** target in-degree,\n",
    "- **L:** shortest path length between source and target,\n",
    "- **S:** name of source article,\n",
    "- **O:** outcome, i.e., the binary indicator that says whether the game was successfully finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=\"img/diagram.png\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answering C2.1** The correct causal diagram is presented in choice **B**.   \n",
    "\n",
    "We can elimate C, E, F as there is a effect from T to S, which is not mentioned in the description.   \n",
    "We can elimate D as there is a effect from L to S whereas the inverse is True.   \n",
    "We can eliminate A as there is a effect from O to L, where as the inverse is True."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C2.2** **/Discuss:/** What is potentially misleading about the naive analysis done in task **C1**? In what conceivable ways could the conclusions about the impact of target in-degree **deg(T)** on the outcome **O** be wrong?\n",
    "Could the findings go in the opposite direction of a true direct causal effect? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answering C2.2** In C1, we are not measuring the direct effect, but the total effect, including the effect mediated through the shortest path length. Possible answers: <br>\n",
    "a) C1 is a very naive analysis, could come up with any confounder that could reverse the effect. <br>\n",
    "b) Depending on on the strength and the sign of the mediated impact, since we are measuring the total impact, we could see different results compared to the true direct causal effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C3. A more controlled analysis: matching on source article and shortest path length\n",
    "\n",
    "Let's now understand how the estimated effect of the target in-degree would differ if we controlled for both the source article and the shortest path length between source and target. Construct a set of matched pairs of games. In a matched pair, the two games differ in target in-degree (one has a high in-degree, and the other has a low in-degree), but the source article and the shortest path length are exactly the same.\n",
    "\n",
    "1. Perform exact matching on source article and shortest path length. This means that two candidate games can be matched only if the source article and the shortest path length are exactly the same.   \n",
    "   **Important heads-up**: *Depending on the configuration of your computer, computing the exact matching could take up to 3 minutes of compute time.*\n",
    "2. Among the matched pairs of games, calculate the fraction of games with a high-in-degree target that was successfully finished, and the fraction of games with a low-in-degree target that was successfully finished. Do you observe significant differences, and in what direction? (You may do a statistical test or provide confidence intervals.)\n",
    "3. **/Discuss:/** What do you observe? Are the conclusions different from the conclusions reached in C1? If yes, speculate as to why that is the case. How is the estimation in task C1 different from the estimation in task C3?\n",
    "4. **/Discuss:/** Based on your findings, how should Wikipedia be modified in order to increase the fraction of finished Wikispeedia games?\n",
    "\n",
    "*Hint: How you construct the set of matched pairs is up to you. One way can be to build an unweighted bipartite graph where games with targets with a high in-degree are on one side (the \"treated class\"), and games with targets with a low in-degree are on the other side (the \"control class\"). The edge between two matching candidate games is created only if there is an exact match on the source article and the shortest path length. Based on this bipartite graph, you can then find the optimal matching. However, note that this is neither the required nor the only way.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''your answer/code here''';\n",
    "G_obs = nx.Graph()\n",
    "\n",
    "vs = set(list(zip(paths.source.values, paths.shortest_path_length.values)))\n",
    "match_set = set()\n",
    "\n",
    "max_counter = len(vs)\n",
    "\n",
    "counter = 0\n",
    "\n",
    "tstart = time.time()\n",
    "for source, min_dist in vs:\n",
    "    counter += 1\n",
    "    if counter % 1000 == 0:\n",
    "        print(counter/max_counter)\n",
    "\n",
    "    finished = paths[(paths.in_degree_binary_target == True) & \n",
    "                     (paths.source == source) &\n",
    "                     (paths.shortest_path_length == min_dist)]\n",
    "\n",
    "    unfinished = paths[(paths.in_degree_binary_target == False) & \n",
    "                       (paths.source == source) &\n",
    "                       (paths.shortest_path_length == min_dist)]\n",
    "    \n",
    "    tmp_edges = []\n",
    "    \n",
    "    for i, f in zip(finished.index, finished.target):\n",
    "        for j, u in zip(unfinished.index, unfinished.target):\n",
    "            if f != u:\n",
    "                G_obs.add_edge(i, j)\n",
    "                match_set.add((i,j))\n",
    "tend = time.time()\n",
    "print(\"Time taken: \", tend-tstart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching = nx.maximal_matching(G_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_games_match_cands = {}; low_games_match_cands = {}\n",
    "for (u,v) in match_set:\n",
    "    if u not in high_games_match_cands:\n",
    "        high_games_match_cands[u] = [v]\n",
    "    else:\n",
    "        high_games_match_cands[u].append(v)\n",
    "    if v not in low_games_match_cands:\n",
    "        low_games_match_cands[v] = [u]\n",
    "    else:\n",
    "        low_games_match_cands[v].append(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(high_games_match_cands), len(low_games_match_cands))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_low_games_match_cands = set(low_games_match_cands.keys())\n",
    "set_high_games_match_cands = set(high_games_match_cands.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'#Matched pairs: {len(matching)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C3.2: Note that a statistical significance test (any meaningful test or bootstrapped CIs) is expected from students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntreated = paths.loc[np.array([v[0] if ~paths.loc[v[0], \"in_degree_binary_target\"] else v[1] for v in matching])]\n",
    "treated = paths.loc[np.array([v[0] if paths.loc[v[0], \"in_degree_binary_target\"] else v[1] for v in matching])]\n",
    "ate = treated.finished.values.astype(int) - ntreated.finished.values.astype(int)\n",
    "print(np.mean(ate),  stats.sem(ate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answering C3:**\n",
    "\n",
    "1. **C3.2:** After matching on source page and shortest path, games with high in-degree source are 14.74% more likely to be finished\n",
    "2. **C3.3:** These differences are smaller compared to how they were before matching, meaning that a lot of the difference can be explained with the mediation through source and the shortest path. However, the direct effect of target in-degree is still significant.\n",
    "3. **C3.4 (Recommendation):** Modify articles to have more high quality incoming links into them, thereby rendering them as better targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
