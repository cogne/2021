{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADA final exam (winter semester 2019/2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A friend of yours wants to start a YouTube channel and ideally earn some money via ads. However, there are so many channels and videos out there that your friend has no idea where to even start. Fortunately, they know that you have taken ADA and think you might help them out by analyzing the videos that are currently on YouTube."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data you are provided with is a subset of YouTube videos, with videos from some of the giant channels in two categories: \"Gaming\" and \"How-to & Style\", which are the categories your friend is choosing between. The dataset contains a lot of videos, with data on those videos including their titles, their total number of views in 2019, their tags and descriptions, etc. The data is, in gzip-compressed format, contained in the `data/` folder, as the file `youtube.csv.gz`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three tasks A, B and C are **independent** of each other, and you can solve any combination of them. The exam is designed for more than 3 hours, so don't worry if you don't manage to solve everything; you can still score a 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to run the following two cells to read and prepare the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.sort_values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube = pd.read_csv('data/youtube.csv.gz', compression='gzip')\n",
    "youtube.upload_date = pd.to_datetime(youtube.upload_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row of the dataset corresponds to one video that was uploaded to YouTube. There are 11 columns:\n",
    "'channel', 'upload_date', 'title', 'categories', 'tags', 'duration',\n",
    "       'view_count', 'average_rating', 'height', 'width', 'channel_cat'.\n",
    "- `channel`: The channel (account) on which the video was uploaded.\n",
    "- `upload_date`: The date on which the video was uploaded (Pandas Timestamp object).\n",
    "- `title`: The title of the video.\n",
    "- `tags`: A list of words that describe the video.\n",
    "- `duration`: The duration of the video in seconds.\n",
    "- `view_count`: The number of times the video was watched.\n",
    "- `average_rating`: The average score with which the viewers rated the video (1-5).\n",
    "- `height`: The height of the video in pixels.\n",
    "- `width`: The width of the video in pixels.\n",
    "- `channel_cat`: The category of the channel on which this video was uploaded. This dataset only contains videos from channels from the 'Gaming' and the 'Howto & Style' category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task A: Welcome to the exam!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of Task A refers to the videos that were published between and including 2010 and 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A1: A growing platform?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You would first like to know whether YouTube in general is the right platform to invest time into."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Using the appropriate plot type, plot the number of videos published per year between and including 2010 and 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment** Select only the videos between 2010/2018 (included)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_10_18 = range(2010,2019)\n",
    "youtube['year'] = youtube.upload_date.apply(lambda x : x.year)\n",
    "youtube_10_18 = youtube[youtube.year.isin(range_10_18)]\n",
    "youtube_10_18.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_10_18.groupby(by='year').agg(\"count\").reset_index().plot.bar(x='year', y='channel', rot=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Now for each year, plot the number of channels that have been created between the beginning of 2010 and the end of that year. A channel is considered to be created at the time at which they upload their first video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = youtube[~youtube.channel.isin(youtube.channel[youtube.upload_date < np.datetime64('2010-01-01')].unique())]\n",
    "#youtube_cat = new_channels[(new_channels.channel_cat.isin(['Gaming', 'Howto & Style']))&(new_channels.year.isin(list(range_10_18)))][['channel','year']]\n",
    "#youtube_cat.sort_values(by ='year').drop_duplicates(subset='channel', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df.loc[(df.channel_cat.apply(lambda x: x in ['Gaming', 'Howto & Style']))&(df.year <= 2018) & (df.year >= 2010), ['year', 'channel']]\\\n",
    "                                    .sort_values('year')\\\n",
    "                                    .drop_duplicates(subset='channel', keep='first')\\\n",
    "                                    .groupby('year').count() + \\\n",
    "                                    pd.DataFrame(data={'channel': [0]*(2018-2010+1),'year':list(range(2010,2018+1))}).set_index('year')\\\n",
    "                                    .fillna(0).cumsum()\n",
    "\n",
    "df_cleaned.plot(kind='bar')\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('channels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Normalize the number of videos published each year by the number of channels that have been created between the beginning of 2010 and the end of that year, and plot these quantities. Do seperate plots for gaming channels, how-to channels, and both together. Can you conclude from the plot that both gaming and how-to channels have been becoming less and less active recently? Why, or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. compute mean number of number of channel between 2010 and year-x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = youtube_10_18.sort_values(by='upload_date', ascending=True)\\\n",
    "                .drop_duplicates(subset='channel', keep='first')\\\n",
    "                .groupby(by='year').agg(count_=pd.NamedAgg(column=\"channel\", aggfunc=\"count\")).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [df.count_.values[:i].mean() for i in range(1,10)]\n",
    "mean[0] = 0\n",
    "std = [df.count_.values[:i].std() for i in range(1,10)]\n",
    "std[0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['norm'] = (df.count_-mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2: The one thing we all love: cash money"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your friend is really keen on making money from their YouTube channel through ads and wants you to help them choose the most profitable channel category (Gaming or Howto & Style). The ad profit is directly proportional to the number of views of a video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Since your friend wants to keep producing videos for several years to come, it might also be worth looking at the growth of the two categories.\n",
    "  1. Compute the total number of views in each category per year for the years 2010-2018.\n",
    "  2. Divide the yearly view count by the number of channels that posted a video in each category in each year. Plot these normalized counts.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Your friend's channel will be brand new, so you decide to look more closely at newer channels. For this question and all the following questions in A2, only consider channels that uploaded their first video in  2016 or later. Compute the total number of views in each category and divide it by the number of channels in that category.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The number of views might be very unevenly over the different channels, and channels might upload different numbers of videos.\n",
    "  1. Compute the mean number of views per video for each channel.\n",
    "  2. Compute the mean of these means for each of the two categories. Print these values.\n",
    "  3. Using bootstrapping, compute 95% confidence intervals for these two means. From this analysis, can you draw a recommendation for one of the two categories? Why, or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task B: View forecasting (Machine Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your friend wants to figure out how they can optimize their videos for getting the maximum number of views (without using shocking thumbnails and clickbait titles). In this task, you will build a machine learning (ML) model for predicting the success of a video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B1: Get those shovels out again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube = pd.read_csv('data/youtube.csv.gz', compression='gzip')\n",
    "youtube.upload_date = pd.to_datetime(youtube.upload_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel</th>\n",
       "      <th>upload_date</th>\n",
       "      <th>title</th>\n",
       "      <th>tags</th>\n",
       "      <th>duration</th>\n",
       "      <th>view_count</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "      <th>channel_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PewDiePie</td>\n",
       "      <td>2013-03-04</td>\n",
       "      <td>A NEW ADVENTURE! - Kingdom Hearts (1) w/ Pewds</td>\n",
       "      <td>['lets', 'play', 'horror', 'game', 'walkthroug...</td>\n",
       "      <td>1126.0</td>\n",
       "      <td>2541550.0</td>\n",
       "      <td>4.886102</td>\n",
       "      <td>720.0</td>\n",
       "      <td>1280.0</td>\n",
       "      <td>Gaming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PewDiePie</td>\n",
       "      <td>2013-03-04</td>\n",
       "      <td>SAVING PRIVATE PEWDS - Conker's Bad Fur Day (15)</td>\n",
       "      <td>['lets', 'play', 'horror', 'game', 'walkthroug...</td>\n",
       "      <td>903.0</td>\n",
       "      <td>1727646.0</td>\n",
       "      <td>4.951531</td>\n",
       "      <td>720.0</td>\n",
       "      <td>1280.0</td>\n",
       "      <td>Gaming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PewDiePie</td>\n",
       "      <td>2013-03-04</td>\n",
       "      <td>THE WORST SCARE! - Amnesia: Rain (4)</td>\n",
       "      <td>['lets', 'play', 'horror', 'game', 'walkthroug...</td>\n",
       "      <td>806.0</td>\n",
       "      <td>1402747.0</td>\n",
       "      <td>4.962706</td>\n",
       "      <td>720.0</td>\n",
       "      <td>1280.0</td>\n",
       "      <td>Gaming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PewDiePie</td>\n",
       "      <td>2013-03-03</td>\n",
       "      <td>Nova / Sp00n / Cry / Pewds - Worms Revolution ...</td>\n",
       "      <td>['lets', 'play', 'horror', 'game', 'walkthroug...</td>\n",
       "      <td>909.0</td>\n",
       "      <td>4348296.0</td>\n",
       "      <td>4.937665</td>\n",
       "      <td>720.0</td>\n",
       "      <td>1280.0</td>\n",
       "      <td>Gaming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PewDiePie</td>\n",
       "      <td>2013-03-03</td>\n",
       "      <td>SEXIEST HORROR EVER - Amnesia: Rain (3)</td>\n",
       "      <td>['lets', 'play', 'horror', 'game', 'walkthroug...</td>\n",
       "      <td>834.0</td>\n",
       "      <td>1410659.0</td>\n",
       "      <td>4.957545</td>\n",
       "      <td>720.0</td>\n",
       "      <td>1280.0</td>\n",
       "      <td>Gaming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139502</th>\n",
       "      <td>cutepolish</td>\n",
       "      <td>2010-02-23</td>\n",
       "      <td>Easy Bride Wedding Nails</td>\n",
       "      <td>['easy', 'makeup', 'beauty', 'fashion']</td>\n",
       "      <td>201.0</td>\n",
       "      <td>284147.0</td>\n",
       "      <td>4.608439</td>\n",
       "      <td>480.0</td>\n",
       "      <td>640.0</td>\n",
       "      <td>Howto &amp; Style</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139503</th>\n",
       "      <td>cutepolish</td>\n",
       "      <td>2010-02-22</td>\n",
       "      <td>Purple Flower Nails</td>\n",
       "      <td>['easy', 'makeup', 'beauty', 'fashion', 'tutor...</td>\n",
       "      <td>180.0</td>\n",
       "      <td>136278.0</td>\n",
       "      <td>4.638451</td>\n",
       "      <td>480.0</td>\n",
       "      <td>640.0</td>\n",
       "      <td>Howto &amp; Style</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139504</th>\n",
       "      <td>cutepolish</td>\n",
       "      <td>2010-02-21</td>\n",
       "      <td>Domo Kun Nails</td>\n",
       "      <td>['easy', 'makeup', 'beauty', 'fashion']</td>\n",
       "      <td>277.0</td>\n",
       "      <td>228384.0</td>\n",
       "      <td>4.836411</td>\n",
       "      <td>480.0</td>\n",
       "      <td>640.0</td>\n",
       "      <td>Howto &amp; Style</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139505</th>\n",
       "      <td>cutepolish</td>\n",
       "      <td>2010-02-20</td>\n",
       "      <td>Easy Plaid Nails</td>\n",
       "      <td>['easy', 'makeup', 'beauty', 'fashion']</td>\n",
       "      <td>174.0</td>\n",
       "      <td>247053.0</td>\n",
       "      <td>4.855700</td>\n",
       "      <td>480.0</td>\n",
       "      <td>640.0</td>\n",
       "      <td>Howto &amp; Style</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139506</th>\n",
       "      <td>cutepolish</td>\n",
       "      <td>2010-02-20</td>\n",
       "      <td>Hime Gyaru Nails</td>\n",
       "      <td>['makeup', 'beauty', 'fashion']</td>\n",
       "      <td>329.0</td>\n",
       "      <td>331964.0</td>\n",
       "      <td>4.462126</td>\n",
       "      <td>480.0</td>\n",
       "      <td>640.0</td>\n",
       "      <td>Howto &amp; Style</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>139507 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           channel upload_date  \\\n",
       "0        PewDiePie  2013-03-04   \n",
       "1        PewDiePie  2013-03-04   \n",
       "2        PewDiePie  2013-03-04   \n",
       "3        PewDiePie  2013-03-03   \n",
       "4        PewDiePie  2013-03-03   \n",
       "...            ...         ...   \n",
       "139502  cutepolish  2010-02-23   \n",
       "139503  cutepolish  2010-02-22   \n",
       "139504  cutepolish  2010-02-21   \n",
       "139505  cutepolish  2010-02-20   \n",
       "139506  cutepolish  2010-02-20   \n",
       "\n",
       "                                                    title  \\\n",
       "0          A NEW ADVENTURE! - Kingdom Hearts (1) w/ Pewds   \n",
       "1        SAVING PRIVATE PEWDS - Conker's Bad Fur Day (15)   \n",
       "2                    THE WORST SCARE! - Amnesia: Rain (4)   \n",
       "3       Nova / Sp00n / Cry / Pewds - Worms Revolution ...   \n",
       "4                 SEXIEST HORROR EVER - Amnesia: Rain (3)   \n",
       "...                                                   ...   \n",
       "139502                           Easy Bride Wedding Nails   \n",
       "139503                                Purple Flower Nails   \n",
       "139504                                     Domo Kun Nails   \n",
       "139505                                   Easy Plaid Nails   \n",
       "139506                                   Hime Gyaru Nails   \n",
       "\n",
       "                                                     tags  duration  \\\n",
       "0       ['lets', 'play', 'horror', 'game', 'walkthroug...    1126.0   \n",
       "1       ['lets', 'play', 'horror', 'game', 'walkthroug...     903.0   \n",
       "2       ['lets', 'play', 'horror', 'game', 'walkthroug...     806.0   \n",
       "3       ['lets', 'play', 'horror', 'game', 'walkthroug...     909.0   \n",
       "4       ['lets', 'play', 'horror', 'game', 'walkthroug...     834.0   \n",
       "...                                                   ...       ...   \n",
       "139502            ['easy', 'makeup', 'beauty', 'fashion']     201.0   \n",
       "139503  ['easy', 'makeup', 'beauty', 'fashion', 'tutor...     180.0   \n",
       "139504            ['easy', 'makeup', 'beauty', 'fashion']     277.0   \n",
       "139505            ['easy', 'makeup', 'beauty', 'fashion']     174.0   \n",
       "139506                    ['makeup', 'beauty', 'fashion']     329.0   \n",
       "\n",
       "        view_count  average_rating  height   width    channel_cat  \n",
       "0        2541550.0        4.886102   720.0  1280.0         Gaming  \n",
       "1        1727646.0        4.951531   720.0  1280.0         Gaming  \n",
       "2        1402747.0        4.962706   720.0  1280.0         Gaming  \n",
       "3        4348296.0        4.937665   720.0  1280.0         Gaming  \n",
       "4        1410659.0        4.957545   720.0  1280.0         Gaming  \n",
       "...            ...             ...     ...     ...            ...  \n",
       "139502    284147.0        4.608439   480.0   640.0  Howto & Style  \n",
       "139503    136278.0        4.638451   480.0   640.0  Howto & Style  \n",
       "139504    228384.0        4.836411   480.0   640.0  Howto & Style  \n",
       "139505    247053.0        4.855700   480.0   640.0  Howto & Style  \n",
       "139506    331964.0        4.462126   480.0   640.0  Howto & Style  \n",
       "\n",
       "[139507 rows x 10 columns]"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "youtube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For the prediction model, use all rows of the dataset, but keep only the following columns: `view_count, channel, upload_date, duration, average_rating, height, width`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>view_count</th>\n",
       "      <th>channel</th>\n",
       "      <th>upload_date</th>\n",
       "      <th>duration</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2541550.0</td>\n",
       "      <td>PewDiePie</td>\n",
       "      <td>2013-03-04</td>\n",
       "      <td>1126.0</td>\n",
       "      <td>4.886102</td>\n",
       "      <td>720.0</td>\n",
       "      <td>1280.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1727646.0</td>\n",
       "      <td>PewDiePie</td>\n",
       "      <td>2013-03-04</td>\n",
       "      <td>903.0</td>\n",
       "      <td>4.951531</td>\n",
       "      <td>720.0</td>\n",
       "      <td>1280.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1402747.0</td>\n",
       "      <td>PewDiePie</td>\n",
       "      <td>2013-03-04</td>\n",
       "      <td>806.0</td>\n",
       "      <td>4.962706</td>\n",
       "      <td>720.0</td>\n",
       "      <td>1280.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4348296.0</td>\n",
       "      <td>PewDiePie</td>\n",
       "      <td>2013-03-03</td>\n",
       "      <td>909.0</td>\n",
       "      <td>4.937665</td>\n",
       "      <td>720.0</td>\n",
       "      <td>1280.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1410659.0</td>\n",
       "      <td>PewDiePie</td>\n",
       "      <td>2013-03-03</td>\n",
       "      <td>834.0</td>\n",
       "      <td>4.957545</td>\n",
       "      <td>720.0</td>\n",
       "      <td>1280.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   view_count    channel upload_date  duration  average_rating  height   width\n",
       "0   2541550.0  PewDiePie  2013-03-04    1126.0        4.886102   720.0  1280.0\n",
       "1   1727646.0  PewDiePie  2013-03-04     903.0        4.951531   720.0  1280.0\n",
       "2   1402747.0  PewDiePie  2013-03-04     806.0        4.962706   720.0  1280.0\n",
       "3   4348296.0  PewDiePie  2013-03-03     909.0        4.937665   720.0  1280.0\n",
       "4   1410659.0  PewDiePie  2013-03-03     834.0        4.957545   720.0  1280.0"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_columns = ['view_count', 'channel', 'upload_date', 'duration', 'average_rating', 'height', 'width']\n",
    "youtube = youtube[mask_columns]\n",
    "youtube.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Extract the upload year and upload month from the `upload_date` column into the two columns `upload_year` and `upload_month`, and remove `upload_date`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>view_count</th>\n",
       "      <th>channel</th>\n",
       "      <th>upload_year</th>\n",
       "      <th>upload_month</th>\n",
       "      <th>duration</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2541550.0</td>\n",
       "      <td>PewDiePie</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1126.0</td>\n",
       "      <td>4.886102</td>\n",
       "      <td>720.0</td>\n",
       "      <td>1280.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1727646.0</td>\n",
       "      <td>PewDiePie</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>903.0</td>\n",
       "      <td>4.951531</td>\n",
       "      <td>720.0</td>\n",
       "      <td>1280.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1402747.0</td>\n",
       "      <td>PewDiePie</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>806.0</td>\n",
       "      <td>4.962706</td>\n",
       "      <td>720.0</td>\n",
       "      <td>1280.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4348296.0</td>\n",
       "      <td>PewDiePie</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>909.0</td>\n",
       "      <td>4.937665</td>\n",
       "      <td>720.0</td>\n",
       "      <td>1280.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1410659.0</td>\n",
       "      <td>PewDiePie</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>834.0</td>\n",
       "      <td>4.957545</td>\n",
       "      <td>720.0</td>\n",
       "      <td>1280.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   view_count    channel  upload_year  upload_month  duration  average_rating  \\\n",
       "0   2541550.0  PewDiePie         2013             3    1126.0        4.886102   \n",
       "1   1727646.0  PewDiePie         2013             3     903.0        4.951531   \n",
       "2   1402747.0  PewDiePie         2013             3     806.0        4.962706   \n",
       "3   4348296.0  PewDiePie         2013             3     909.0        4.937665   \n",
       "4   1410659.0  PewDiePie         2013             3     834.0        4.957545   \n",
       "\n",
       "   height   width  \n",
       "0   720.0  1280.0  \n",
       "1   720.0  1280.0  \n",
       "2   720.0  1280.0  \n",
       "3   720.0  1280.0  \n",
       "4   720.0  1280.0  "
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_columns_2 = ['view_count', 'channel', 'upload_year','upload_month', 'duration', 'average_rating', 'height', 'width']\n",
    "youtube['upload_year'] = youtube.upload_date.apply(lambda x : x.year)\n",
    "youtube['upload_month'] = youtube.upload_date.apply(lambda x : x.month)\n",
    "youtube = youtube[mask_columns_2]\n",
    "youtube.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The entry in the channel column for a video indicates on which channel the video was uploaded. Encode this column via one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#How many new columns ?\n",
    "len(youtube.channel.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>view_count</th>\n",
       "      <th>upload_year</th>\n",
       "      <th>upload_month</th>\n",
       "      <th>duration</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>...</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2541550.0</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1126.0</td>\n",
       "      <td>4.886102</td>\n",
       "      <td>720.0</td>\n",
       "      <td>1280.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1727646.0</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>903.0</td>\n",
       "      <td>4.951531</td>\n",
       "      <td>720.0</td>\n",
       "      <td>1280.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1402747.0</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>806.0</td>\n",
       "      <td>4.962706</td>\n",
       "      <td>720.0</td>\n",
       "      <td>1280.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4348296.0</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>909.0</td>\n",
       "      <td>4.937665</td>\n",
       "      <td>720.0</td>\n",
       "      <td>1280.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1410659.0</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>834.0</td>\n",
       "      <td>4.957545</td>\n",
       "      <td>720.0</td>\n",
       "      <td>1280.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   view_count  upload_year  upload_month  duration  average_rating  height  \\\n",
       "0   2541550.0         2013             3    1126.0        4.886102   720.0   \n",
       "1   1727646.0         2013             3     903.0        4.951531   720.0   \n",
       "2   1402747.0         2013             3     806.0        4.962706   720.0   \n",
       "3   4348296.0         2013             3     909.0        4.937665   720.0   \n",
       "4   1410659.0         2013             3     834.0        4.957545   720.0   \n",
       "\n",
       "    width    0    1    2  ...  185  186  187  188  189  190  191  192  193  \\\n",
       "0  1280.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "1  1280.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "2  1280.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "3  1280.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "4  1280.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   194  \n",
       "0  0.0  \n",
       "1  0.0  \n",
       "2  0.0  \n",
       "3  0.0  \n",
       "4  0.0  \n",
       "\n",
       "[5 rows x 202 columns]"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating instance of one-hot-encoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "# Passing bridge-types-cat column (label encoded values of bridge_types)\n",
    "enc_df = pd.DataFrame(enc.fit_transform(youtube[['channel']]).toarray())\n",
    "# Merge with main df bridge_df on key values\n",
    "youtube_ = youtube.join(enc_df)\n",
    "youtube_ = youtube_.drop(labels='channel', axis=1)\n",
    "youtube_.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Split the data into a train (70%) and a test set (30%) with the appropriate function from sklearn, using 42 as the random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(youtube_, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B2: Who is the most viewed of them all?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Train a ridge regression model (i.e., an L2-regularized linear regression model) on the train set that predicts the view count from the other features. Find and use the optimal regularization parameter $\\alpha$ from the set {0.001, 0.01, 0.1} via 3-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Ridge(), param_grid={'alpha': (0.001, 0.01, 0.1)})"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = train.drop(labels='view_count', axis=1)\n",
    "y_train = train.view_count\n",
    "ridge = Ridge()\n",
    "parameters = {'alpha':(0.001, 0.01, 0.1)}\n",
    "clf = GridSearchCV(ridge, parameters)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01563001, 0.01292539, 0.01266613])"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.cv_results_['mean_score_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00505938, 0.00026913, 0.0002349 ])"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.cv_results_['std_score_time']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Report the mean absolute error that the model makes on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test.drop(labels='view_count', axis=1)\n",
    "y_test = test.view_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1444649.5039951713"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_test, clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B3: Checking our ambitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve performance, you want to make the task of the ML model easier and turn it into a classification task. Now it only has to predict whether a video has a high view count (defined as being larger than the median of the view counts in the training set) or a low view count (defined as being smaller or equal to the median of the view counts in the training set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Train a logistic regression model for this classification task. Find and use the optimal regularization parameter C (as defined in scikit-learn's documentation) from the set {1, 10, 100} via 3-fold cross validation. Use the random seed 42. _Hint_: If you get a warning about the training algorithm failing to converge, increase the maximum number of training iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = y_train.median()\n",
    "y_train_binary = (y_train > threshold).astype(int)\n",
    "y_test_binary = (y_test > threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=LogisticRegression(max_iter=1000, random_state=42),\n",
       "             param_grid={'C': (1, 10, 100)})"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit = LogisticRegression(random_state=42, max_iter=1000)\n",
    "parameters = {'C':(1,10,100)}\n",
    "clf = GridSearchCV(logit, parameters)\n",
    "clf.fit(X_train, y_train_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03399649, 0.03553305, 0.035533  ])"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.cv_results_['mean_score_time']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Compute the accuracy of the logistic regression model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7480228418512412"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B4: ...something's not right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are satisfied with the model performance. In fact, you are a bit surprised at how good the model is given the relatively little amount of information about the videos. So you take a closer look at the features and realize that the (one-hot-encoded) channel feature does not make sense for the application that your friend has in mind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Why does the channel feature not make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment** \n",
    "\n",
    "Because the channel is too specific and it is more an name than a feature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Train another logistic regression model with all the features from B3 except the one-hot-encoded channel. Use again 42 as the seed for the train test split and perform the same hyperparameter optimization as in B3. How does the model performance change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41853, 6)"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_train[['upload_year','upload_month','duration', 'average_rating','height','width']]\n",
    "X_test = X_test[['upload_year','upload_month','duration', 'average_rating','height','width']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=LogisticRegression(max_iter=1000, random_state=42),\n",
       "             param_grid={'C': (1, 10, 100)})"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit = LogisticRegression(random_state=42, max_iter=1000)\n",
    "parameters = {'C':(1,10,100)}\n",
    "clf = GridSearchCV(logit, parameters)\n",
    "clf.fit(X_train, y_train_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.002772  , 0.00269728, 0.00259695])"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.cv_results_['mean_score_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6048311948964232"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**\n",
    "\n",
    "The performance is lower for this set up. Also we can notice that it is not more C=10 but C=1 for the parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B5: \"We kinda forgot about categories.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On second thought, there is actually one feature that you may use about the channel. Namely, the channel category. The reason this one makes sense might also help you answer B4.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Train and evaluate another logistic regression model (in the same way as in B4 regarding train/test split and hyperparameter) that additionally includes the one-hot-encoded channel category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_ml = youtube_ml[['view_count', 'duration', 'average_rating', 'height', 'width', 'u_year', 'u_month']]\n",
    "youtube_ml_gaming = youtube_ml[youtube['channel_cat'] == 'Gaming']\n",
    "youtube_ml_howto = youtube_ml[youtube['channel_cat'] == 'Howto & Style']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gaming, test_gaming = train_test_split(youtube_ml_gaming, test_size=0.3, random_state=1)\n",
    "X_train_gaming = train_gaming.drop(columns=['view_count'])\n",
    "y_train_gaming = train_gaming['view_count']\n",
    "X_test_gaming = test_gaming.drop(columns=['view_count'])\n",
    "y_test_gaming = test_gaming['view_count']\n",
    "\n",
    "train_howto, test_howto = train_test_split(youtube_ml_howto, test_size=0.3, random_state=1)\n",
    "X_train_howto = train_howto.drop(columns=['view_count'])\n",
    "y_train_howto = train_howto['view_count']\n",
    "X_test_howto = test_howto.drop(columns=['view_count'])\n",
    "y_test_howto = test_howto['view_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_binary_gaming = (y_train_gaming > y_train_gaming.median()).astype(int)\n",
    "y_test_binary_gaming = (y_test_gaming > y_train_gaming.median()).astype(int)\n",
    "y_train_binary_howto = (y_train_howto > y_train_howto.median()).astype(int)\n",
    "y_test_binary_howto = (y_test_howto > y_train_howto.median()).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6309280826029535"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train_gaming, y_train_binary_gaming)\n",
    "clf.score(X_test_gaming, y_test_binary_gaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6413494604529824"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train_howto, y_train_binary_howto)\n",
    "clf.score(X_test_howto, y_test_binary_howto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The dynamics of the two categories might differ a lot, and the two communities might value different properties of a video differently. For instance, for one community, a long duration might be more important, for the other one, a large picture width. Thus, having only a single weight for, e.g., the duration of a video, might not give the best results. Is there something smarter that you can do than simply including the category as a single one-hot-encoded feature to improve the classification performance? Implement your idea and compare the accuracy on the test set with that of the first model (from task B5.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task C: A map of the channels (Graphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your friend wants to map out the channels and represent their similarities. For this purpose, we have created two undirected and unweighted graphs for you, where in each graph, each channel has a node and similar channels have edges connecting them. In one graph, the similarity between two channels is based on how similar their video descriptions are, while in the other, the similarity is based on how similar their video tags are. We will call the former $G_{text}$ and the latter $G_{tags}$. You will be analyzing the two graphs loaded by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx import from_numpy_array\n",
    "import json\n",
    "g_text_adj = np.loadtxt(open('data/g_text_adj.csv', 'r'), delimiter=',', skiprows=0)\n",
    "g_tags_adj = np.loadtxt(open('data/g_tags_adj.csv', 'r'), delimiter=',', skiprows=0)\n",
    "channel_to_index = json.load(open('data/channel_indices.json', 'r'))\n",
    "g_text = from_numpy_array(g_text_adj)\n",
    "g_tags = from_numpy_array(g_tags_adj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C1: Does YouTube have a content diversity problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For each graph, calculate its diameter (i.e., the largest shortest-path length, where the maximization is done over all node pairs). What difference do you see? _Hint_: Don't worry if you get an error, just read the error message carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_text = nx.diameter(g_text, e=None, usebounds=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d_tags = nx.diameter(g_tags, e=None, usebounds=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment** For the G_text, the diameter is equal to 2, for the G_tags there is no diameters since the graph is not connected and would have an infinite diameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What does the diameter of $G_{text}$ say about the diversity of the channels’ contents? How about the diameter of $G_{tags}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment $G_{text}$** \n",
    "\n",
    "Diameter = largest shortest-path length\n",
    "\n",
    "It seems that for $G_{text}$, all the nodes are connected almost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('In G_text, there is %d edges and %d nodes.'% (len(g_text.edges()), len(g_text.nodes())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a graph is fully connected it as n(n-1)/2 edges where n is the number of edges. In our case, for 195 it would be 18'915 which is almost the number that we have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment $G_{tags}$**\n",
    "\n",
    "When we tried to find the diameter of the graph, we raised an error. It seems that the graph is not connected so for some nodes, there is no paths between them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Based on what you have calculated, which one has greater diversity: descriptions used by channels, or tags used by channels? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment** \n",
    "\n",
    "$G_{text}$ has low diversity since the diameter is small, while $G_{tags}$ doesn't tell us much because its diameter is undefined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Imagine that you want to **compare** content diversity between two sets of channels (i.e., you want to see which set of channels has more diverse content), and you have calculated a tag-based graph for each set. Do you think the diameter is a good measure for doing the comparison? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**\n",
    "\n",
    "No, because as we saw, the diameter can end up being undefined for both, which implies that both are diverse but does not provide much of a comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Back to our own two graphs. Based on $G_{text}$, for each category of channels, which channel is the one most representative of the contents of all channels in that category? In other words, for each category, if you needed to provide a summary of all channels in the category via one channel, which channel would you choose? Show us (us being the exam designers and your friend) the descriptions of this channel’s two most-viewed videos. What metric did you use for this purpose? Explain your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centrality_node = nx.degree_centrality(g_text)\n",
    "centrality_node = {k: v for k, v in sorted(centrality_node.items(), key=lambda item: -item[1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centrality_node[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_to_index['Desi Perkins']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube[youtube.channel == 'Desi Perkins']['channel_cat'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube[youtube.channel == 'Desi Perkins']['view_count'].sum(), youtube[youtube.channel == 'Desi Perkins']['view_count'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**\n",
    "\n",
    "Since the degree centrality for a node v is the fraction of nodes it is connected to, the one with the biggest score in his category will be the one the most representative.\n",
    "\n",
    "The node with the highest centrality score is the node 1.\n",
    "\n",
    "This node correspond to the channel Desi Perkins and it represents the category How to & Style. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C2: Going back to categories again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We want to use the two graphs to cluster channels from the same category together, and we want to compare their effectiveness at doing so. Use Kernighan-Lin bisection in the networkx package (`networkx.algorithms.community.kernighan_lin_bisection`) to divide each graph into two communities. Use 42 as the random seed. For each graph, show how many members of each category fall into each of the two communities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "communities = nx.algorithms.community.kernighan_lin_bisection(g_text,seed=42)\n",
    "print('In G_text, there is %d nodes in the category \"How to & Style\" ' % (len(communities[0])))\n",
    "print('In G_text, there is %d nodes in the category \"Gaming\" ' % (len(communities[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "communities_ = nx.algorithms.community.kernighan_lin_bisection(g_tags,seed=42)\n",
    "print('In G_text, there is %d nodes in the category \"How to & Style\" ' % (len(communities_[0])))\n",
    "print('In G_text, there is %d nodes in the category \"Gaming\" ' % (len(communities_[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. If one of these graphs were ideal for this clustering task, what would the resulting communities look like? If it were the absolute worst possible graph for the task, what would the resulting communities look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ideal Graph: would have two distincts communities with maybe some outliers.\n",
    "- Worst Graph: would have a lot of communities with only a few number of edges in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Calculate the probability $P(community|category)$ for each community and category within each graph. Design a metric, using the four $P(community|category)$ values in a graph, whose value would be 1 for the ideal graph and 0 for the worst graph. Calculate this metric for both graphs and compare the two. What do the results say about how representative tags and descriptions are regarding the channel categories? Are tags better suited, or descriptions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_community_nodes = list(communities[0])\n",
    "real_hs_community = youtube[youtube.channel_cat == \"Howto & Style\"]\n",
    "real_hs_community['index']= real_hs_community.channel.apply(lambda x : channel_to_index[x])\n",
    "\n",
    "gaming_community_nodes = list(communities[1])\n",
    "real_gaming_community = youtube[youtube.channel_cat == \"Gaming\"]\n",
    "real_gaming_community['index']= real_gaming_community.channel.apply(lambda x : channel_to_index[x])\n",
    "\n",
    "p_hs = real_hs_community.shape[0]/youtube.shape[0] \n",
    "p_gaming = real_gaming_community.shape[0]/youtube.shape[0]\n",
    "\n",
    "p_c0 = len(hs_community_nodes)/(len(hs_community_nodes)+len(gaming_community_nodes))\n",
    "p_c1 = len(gaming_community_nodes)/(len(hs_community_nodes)+len(gaming_community_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_hs_c0 = len(set(hs_community_nodes).intersection(set(real_hs_community['index'].values)))/len(hs_community_nodes)\n",
    "p_gaming_c0 = len(set(hs_community_nodes).intersection(set(real_gaming_community['index'].values)))/len(hs_community_nodes)\n",
    "\n",
    "p_hs_c1 = len(set(gaming_community_nodes).intersection(set(real_hs_community['index'].values)))/len(gaming_community_nodes)\n",
    "p_gaming_c1 = len(set(gaming_community_nodes).intersection(set(real_gaming_community['index'].values)))/len(gaming_community_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_c0_hs = (p_hs_c0*p_c0)/p_hs\n",
    "p_c0_hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_c1_hs = (p_hs_c1*p_c1)/p_hs\n",
    "p_c1_hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_c0_gaming = (p_gaming_c0*p_c0)/p_gaming\n",
    "p_c0_gaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_c1_gaming = (p_gaming_c1*p_c1)/p_gaming\n",
    "p_c1_gaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. The Kernighan-Lin bisection you used above performs a min-edge cut: It attempts to partition the nodes of the graph into two sets of almost-equal size by deleting as few edges as possible. It starts off by creating a random partition of the nodes of the graph into two sets A and B that are almost equal in size, and then iteratively and in a greedy fashion moves nodes between A and B to reduce the number of edges between A and B. Show at least one toy example of a graph where the initialization could also be the final result. (Hint: Think of how, as we explained, the bisection algorithm relies on a minimum edge cut with a random initialization; under what circumstances could the original A and B be the best partition given that graph?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
