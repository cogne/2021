{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADA final exam (winter semester 2019/2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A friend of yours wants to start a YouTube channel and ideally earn some money via ads. However, there are so many channels and videos out there that your friend has no idea where to even start. Fortunately, they know that you have taken ADA and think you might help them out by analyzing the videos that are currently on YouTube."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data you are provided with is a subset of YouTube videos, with videos from some of the giant channels in two categories: \"Gaming\" and \"How-to & Style\", which are the categories your friend is choosing between. The dataset contains a lot of videos, with data on those videos including their titles, their total number of views in 2019, their tags and descriptions, etc. The data is, in gzip-compressed format, contained in the `data/` folder, as the file `youtube.csv.gz`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three tasks A, B and C are **independent** of each other, and you can solve any combination of them. The exam is designed for more than 3 hours, so don't worry if you don't manage to solve everything; you can still score a 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to run the following two cells to read and prepare the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube = pd.read_csv('data/youtube.csv.gz', compression='gzip')\n",
    "youtube.upload_date = pd.to_datetime(youtube.upload_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row of the dataset corresponds to one video that was uploaded to YouTube. There are 11 columns:\n",
    "'channel', 'upload_date', 'title', 'categories', 'tags', 'duration',\n",
    "       'view_count', 'average_rating', 'height', 'width', 'channel_cat'.\n",
    "- `channel`: The channel (account) on which the video was uploaded.\n",
    "- `upload_date`: The date on which the video was uploaded (Pandas Timestamp object).\n",
    "- `title`: The title of the video.\n",
    "- `tags`: A list of words that describe the video.\n",
    "- `duration`: The duration of the video in seconds.\n",
    "- `view_count`: The number of times the video was watched.\n",
    "- `average_rating`: The average score with which the viewers rated the video (1-5).\n",
    "- `height`: The height of the video in pixels.\n",
    "- `width`: The width of the video in pixels.\n",
    "- `channel_cat`: The category of the channel on which this video was uploaded. This dataset only contains videos from channels from the 'Gaming' and the 'Howto & Style' category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import Ridge, LinearRegression, LogisticRegressionCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task A: Welcome to the exam!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of Task A refers to the video that were published between and including 2010 and 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A1: A growing platform?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You would first like to know whether YouTube in general is the right platform to invest time into."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Using the appropriate plot type, plot the number of videos published per year between and including 2010 and 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Now for each year, plot the number of channels that have been created between the beginning of 2010 and the end of that year. A channel is considered to be created at the time at which they upload their first video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Normalize the number of videos published each year by the number of channels that have been created between the beginning of 2010 and the end of that year, and plot these quantities. Do seperate plots for gaming channels, how-to channels, and both together. Can you conclude from the plot that both gaming and how-to channels have been becoming less and less active recently? Why, or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube['upload_year'] = youtube.upload_date.apply(lambda x: x.year)\n",
    "youtube['upload_month'] = youtube.upload_date.apply(lambda x: x.month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_per_year(df, categories, col, year_start, year_end, what='count',\n",
    "                       keep_duplicates=True, duplicate_cols=None, cumulative=False):\n",
    "    df_cleaned = df.loc[(df.channel_cat.apply(lambda x: x in categories))&\n",
    "    (df.upload_year <= year_end) & (df.upload_year >= year_start), \n",
    "        ['upload_year', col]].sort_values('upload_year')\n",
    "    if not keep_duplicates:\n",
    "        df_cleaned = df_cleaned.drop_duplicates(subset=duplicate_cols, keep='first')\n",
    "    if what == 'count':\n",
    "        result_df = df_cleaned.groupby('upload_year').count() + \\\n",
    "            pd.DataFrame(data={col: [0]*(year_end-year_start+1), \n",
    "            'upload_year':list(range(year_start,year_end+1))}).set_index('upload_year')\n",
    "    elif what == 'mean':\n",
    "        result_df = df_cleaned.groupby('upload_year').mean()\n",
    "    elif what == 'sum':\n",
    "        result_df = df_cleaned.groupby('upload_year').sum()\n",
    "    if cumulative:\n",
    "        result_df = result_df.fillna(0).cumsum()\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube.channel_cat.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats_chosen = ['Gaming', 'Howto & Style']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A1.1\n",
    "The appropriate plot type is a bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vids_per_year = [calculate_per_year(youtube, [x], 'title', 2010, 2018) for x in cats_chosen]\n",
    "vids_per_year.append(calculate_per_year(youtube, cats_chosen, 'title', 2010, 2018))\n",
    "titles = cats_chosen + ['Both']\n",
    "#fig = plt.figure(figsize=(20,10))\n",
    "plt.bar(x=vids_per_year[2].index, height=vids_per_year[2].title)\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('videos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't keep channels that have existed before 2010.\n",
    "new_channels = youtube[~youtube.channel.isin(youtube.channel[youtube.upload_date < np.datetime64('2010-01-01')].unique())]\n",
    "channels_per_year = calculate_per_year(new_channels, cats_chosen, 'channel', 2010, 2018, \n",
    "                                        keep_duplicates=False, duplicate_cols=['channel'],\n",
    "                                        cumulative=True)\n",
    "\n",
    "channels_per_year.plot(kind='bar')\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('channels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "for i in range(len(vids_per_year)):\n",
    "    ax = fig.add_subplot(1,3,i+1)\n",
    "    current_df = vids_per_year[i].join(channels_per_year).\\\n",
    "                    assign(avg_count=lambda x: x['title']/x['channel'])[['avg_count']]\n",
    "    ax.bar(x=current_df.index, height=current_df.avg_count)\n",
    "    ax.set_title(titles[i])\n",
    "    ax.set_xlabel('year')\n",
    "    ax.set_ylabel('per-channel video average')\n",
    "#    ax.set_ylim([0,21000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So together, it seems that activity has gone down, but truth is, gaming has gone down dramatically while howto has risen in recent years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2: The one thing we all love: cash money"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your friend is really keen on making money from their YouTube channel through ads and wants you to help them choose the most profitable channel category (Gaming or Howto & Style). The ad profit is directly proportional to the number of views of a video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Since your friend wants to keep producing videos for several years to come, it might also be worth looking at the growth of the two categories.\n",
    "  1. Compute the total number of views in each category per year for the years 2010-2018.\n",
    "  2. Divide the yearly view count by the number of channels that posted a video in each category in each year. Plot these normalized counts.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. It takes some time for channels to become popular. For this question and all the following questions in A2, only consider channels that uploaded their first video in  2016 or later. Compute the total number of views in each category and divide it by the number of channels in that category.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The number of views might be very unevenly over the different channels, and channels might upload different numbers of videos.\n",
    "  1. Compute the mean number of views per video for each channel.\n",
    "  2. Compute the mean of these means for each of the two categories. Print these values.\n",
    "  3. Using bootstrapping, compute 95% confidence intervals for these two means. From this analysis, can you draw a recommendation for one of the two categories? Why, or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2.1\n",
    "### (A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube.loc[youtube.channel_cat == 'Gaming'].view_count.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube.loc[youtube.channel_cat == 'Howto & Style'].view_count.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat1_views = calculate_per_year(youtube, ['Gaming'], 'view_count', 2010, 2018, what='sum')\n",
    "# cat1_views.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat2_views = calculate_per_year(youtube, ['Howto & Style'], 'view_count', 2010, 2018, what='sum')\n",
    "# cat2_views.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels_cat1 = calculate_per_year(youtube, ['Gaming'], 'channel', 2010, 2018, 'count', keep_duplicates=False, \n",
    "                          duplicate_cols=['upload_year', 'channel'])\n",
    "channels_cat2 = calculate_per_year(youtube, ['Howto & Style'], 'channel', 2010, 2018, 'count', keep_duplicates=False, \n",
    "                          duplicate_cols=['upload_year', 'channel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat1_views.join(channels_cat1).assign(avg_views=lambda x: x['view_count']/x['channel'])[['avg_views']].\\\n",
    "                                plot(kind='bar')\n",
    "plt.title('gaming')\n",
    "plt.ylabel('views/channels')\n",
    "plt.xlabel('upload year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat2_views.join(channels_cat2).assign(avg_views=lambda x: x['view_count']/x['channel'])[['avg_views']].\\\n",
    "                                plot(kind='bar')\n",
    "plt.title('howto and style')\n",
    "plt.ylabel('views/channels')\n",
    "plt.xlabel('upload year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_starts = youtube[['channel', 'upload_year']].groupby('channel').min().reset_index()\n",
    "channels_late = channel_starts.loc[channel_starts.upload_year >= 2016].channel.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels_late"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(youtube.loc[(youtube.channel_cat == 'Howto & Style') & \n",
    "            (youtube.channel.apply(lambda x: x in channels_late))].channel.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(youtube.loc[(youtube.channel_cat == 'Gaming') & \n",
    "             (youtube.channel.apply(lambda x: x in channels_late))].channel.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two are the answers to A2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube.loc[(youtube.channel_cat == 'Howto & Style') & \n",
    "            (youtube.channel.apply(lambda x: x in channels_late))].view_count.sum() / \\\n",
    "            len(youtube.loc[(youtube.channel_cat == 'Howto & Style') & \n",
    "            (youtube.channel.apply(lambda x: x in channels_late))].channel.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube.loc[(youtube.channel_cat == 'Gaming') & \n",
    "            (youtube.channel.apply(lambda x: x in channels_late))].view_count.sum() /\\\n",
    "            len(youtube.loc[(youtube.channel_cat == 'Gaming') & \n",
    "             (youtube.channel.apply(lambda x: x in channels_late))].channel.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the aggregation, per channel. For this aggregation, there are two solutions:\n",
    "* If you consider the effort spent for each video to be similar, then the average.\n",
    "* If you only want to consider the total potential revenue, then the sum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2.3A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "howto_perchannel_new = youtube.loc[(youtube.channel_cat == 'Howto & Style') & \n",
    "            (youtube.channel.apply(lambda x: x in channels_late)), ['view_count', 'channel']].\\\n",
    "            groupby('channel').mean().view_count.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaming_perchannel_new = youtube.loc[(youtube.channel_cat == 'Gaming') & \n",
    "            (youtube.channel.apply(lambda x: x in channels_late)), ['view_count', 'channel']].\\\n",
    "            groupby('channel').mean().view_count.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2.3B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(howto_perchannel_new), np.mean(gaming_perchannel_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2.3C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_bootstrap(data, n=1000):\n",
    "    sample_statistic = [] \n",
    "    for _ in range(n):\n",
    "        sampled_data = np.random.choice(data, size=len(data))  \n",
    "        sample_statistic.append(np.mean(sampled_data))\n",
    "    return (np.percentile(sample_statistic, 2.5), np.percentile(sample_statistic, 97.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[do_bootstrap(howto_perchannel_new), do_bootstrap(gaming_perchannel_new)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference is significant, as the two confidence intervals have no overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task B: View forecasting (Machine Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your friend wants to figure out how they can optimize their videos for getting the maximum number of views (without using shocking thumbnails and clickbait titles). In this task, you will build a machine learning (ML) model for predicting the success of a video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B1: Get those shovels out again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For the prediction model, use all rows of the dataset, but keep only the following columns: `view_count, channel, upload_date, duration, average_rating, height, width`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_columns = ['view_count', 'channel', 'upload_date', 'duration', 'average_rating', 'height', 'width']\n",
    "youtube_ml = youtube[ml_columns].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Extract the upload year and upload month from the `upload_date` column into the two columns `upload_year` and `upload_month`, and remove `upload_date`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_ml['u_year'] = youtube_ml.upload_date.dt.year\n",
    "youtube_ml['u_month'] = youtube_ml.upload_date.dt.month\n",
    "youtube_ml = youtube_ml.drop(columns=['upload_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The entry in the channel column for a video indicates on which channel the video was uploaded. Encode this column via one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_ml = pd.get_dummies(youtube_ml, columns=['channel'], prefix='channel_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Split the data into a train (70%) and a test set (30%) with the appropriate function from sklearn, using 42 as the random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(youtube_ml, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B2: Who is the most viewed of them all?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Train a ridge regression model (i.e., an L2-regularized linear regression model) on the train set that predicts the view count from the other features. Find and use the optimal regularization parameter $\\alpha$ from the set {0.001, 0.01, 0.1} via 3-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(columns=['view_count'])\n",
    "y_train = train['view_count']\n",
    "X_test = test.drop(columns=['view_count'])\n",
    "y_test = test['view_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge()\n",
    "ridge_hyper = {'alpha':(0.001, 0.01, 0.1)}\n",
    "ridge_cv = GridSearchCV(ridge, ridge_hyper, cv=3)\n",
    "ridge_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_cv.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Report the mean absolute error that the model makes on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_test, ridge_cv.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B3: Checking our ambitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve performance, you want to make the task of the ML model easier and turn it into a classification task. Now it only has to predict whether a video has a high view count (defined as being larger than the median of the view counts in the training set) or a low view count (defined as being smaller or equal to the median of the view counts in the training set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Train a logistic regression model for this classification task. Find and use the optimal regularization parameter C (as defined in scikit-learn's documentation) from the set {1, 10, 100} via 3-fold cross validation. Use the random seed 42. _Hint_: If you get a warning about the training algorithm failing to converge, increase the maximum number of training iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = y_train.median()\n",
    "\n",
    "y_train_binary = (y_train > threshold).astype(int)\n",
    "y_test_binary = (y_test > threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = (1, 10, 100)\n",
    "log_reg_cv = LogisticRegressionCV(Cs=Cs, cv=3, random_state=42, max_iter=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_cv.fit(X_train, y_train_binary)\n",
    "opt_C = log_reg_cv.C_[0]\n",
    "opt_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_cv.scores_[1].mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Compute the accuracy of the logistic regression model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_cv.score(X_test, y_test_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B4: ...something's not right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are satisfied with the model performance. In fact, you are a bit surprised at how good the model is given the relatively little amount of information about the videos. So you take a closer look at the features and realize that the (one-hot-encoded) channel feature does not make sense for the application that your friend has in mind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Why does the channel feature not make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though your friend knows who will upload their videos (your friend, of course), no video with this channel is in the training set and thus the corresponding one-hot feature does not exist in the model. Thus they wouldn't be able to use the trained model for predictions on their own videos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Train another logistic regression model with all the features from B3 except the one-hot-encoded channel. Use again 42 as the seed for the train test split and perform the same hyperparameter optimization as in B3. How does the model performance change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_ml = youtube_ml[['view_count', 'duration', 'average_rating', 'height', 'width', 'u_year', 'u_month']]\n",
    "train, test = train_test_split(youtube_ml, test_size=0.3, random_state=42)\n",
    "X_train = train.drop(columns=['view_count'])\n",
    "y_train = train['view_count']\n",
    "X_test = test.drop(columns=['view_count'])\n",
    "y_test = test['view_count']\n",
    "\n",
    "threshold = y_train.median()\n",
    "\n",
    "y_train_binary = (y_train > threshold).astype(int)\n",
    "y_test_binary = (y_test > threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_cv.fit(X_train, y_train_binary)\n",
    "opt_C = log_reg_cv.C_[0]\n",
    "opt_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_cv.score(X_test, y_test_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without the channel, the accuracy drops by 15 percentage points, and is not that far from uniformly random prediction anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B5: \"We kinda forgot about categories.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On second thought, there is actually one feature that you may use about the channel. Namely, the channel category. The reason this one makes sense might also help you answer B4.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Train and evaluate another logistic regression model (in the same way as in B4 regarding train/test split and hyperparameter) that additionally includes the one-hot-encoded channel category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_ml['channel_cat'] = youtube['channel_cat']\n",
    "youtube_ml = pd.get_dummies(youtube_ml, columns=['channel_cat'], prefix='cat_')\n",
    "\n",
    "train, test = train_test_split(youtube_ml, test_size=0.3, random_state=1)\n",
    "X_train = train.drop(columns=['view_count'])\n",
    "y_train = train['view_count']\n",
    "X_test = test.drop(columns=['view_count'])\n",
    "y_test = test['view_count']\n",
    "\n",
    "threshold = y_train.median()\n",
    "\n",
    "y_train_binary = (y_train > threshold).astype(int)\n",
    "y_test_binary = (y_test > threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_cv.fit(X_train, y_train_binary)\n",
    "opt_C = log_reg_cv.C_[0]\n",
    "opt_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_cv.score(X_test, y_test_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The dynamics of the two categories might differ a lot, and the two communities might value different properties of a video differently. For instance, for one community, a long duration might be more important, for the other one, a large picture width. Thus, having only a single weight for, e.g., the duration of a video, might not give the best results. Is there something smarter that you can do than simply including the category as a single one-hot-encoded feature to improve the classification performance? Implement your idea and compare the accuracy on the test set with that of the first model (from task B5.1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We can instead train two separate models, one for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_ml = youtube_ml[['view_count', 'duration', 'average_rating', 'height', 'width', 'u_year', 'u_month']]\n",
    "youtube_ml_gaming = youtube_ml[youtube['channel_cat'] == 'Gaming']\n",
    "youtube_ml_howto = youtube_ml[youtube['channel_cat'] == 'Howto & Style']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gaming, test_gaming = train_test_split(youtube_ml_gaming, test_size=0.3, random_state=1)\n",
    "X_train_gaming = train_gaming.drop(columns=['view_count'])\n",
    "y_train_gaming = train_gaming['view_count']\n",
    "X_test_gaming = test_gaming.drop(columns=['view_count'])\n",
    "y_test_gaming = test_gaming['view_count']\n",
    "\n",
    "train_howto, test_howto = train_test_split(youtube_ml_howto, test_size=0.3, random_state=1)\n",
    "X_train_howto = train_howto.drop(columns=['view_count'])\n",
    "y_train_howto = train_howto['view_count']\n",
    "X_test_howto = test_howto.drop(columns=['view_count'])\n",
    "y_test_howto = test_howto['view_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_binary_gaming = (y_train_gaming > y_train_gaming.median()).astype(int)\n",
    "y_test_binary_gaming = (y_test_gaming > y_train_gaming.median()).astype(int)\n",
    "y_train_binary_howto = (y_train_howto > y_train_howto.median()).astype(int)\n",
    "y_test_binary_howto = (y_test_howto > y_train_howto.median()).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training separate models for the two categories gives slightly better accuracies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First for the gaming category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_cv.fit(X_train_gaming, y_train_binary_gaming)\n",
    "log_reg_cv.score(X_test_gaming, y_test_binary_gaming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then for the howto category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_cv.fit(X_train_howto, y_train_binary_howto)\n",
    "log_reg_cv.score(X_test_howto, y_test_binary_howto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task C: A map of the channels (Graphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your friend wants to map out the channels and represent their similarities. For this purpose, we have created two undirected and unweighted graphs for you, where in each graph, each channel has a node and similar channels have edges connecting them. In one graph, the similarity between two channels is based on how similar their video descriptions are, while in the other, the similarity is based on how similar their video tags are. We will call the former $G_{text}$ and the latter $G_{tags}$. You will be analyzing the two graphs loaded by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx import from_numpy_array\n",
    "import json\n",
    "g_text_adj = np.loadtxt(open('data/g_text_adj.csv', 'r'), delimiter=',', skiprows=0)\n",
    "g_tags_adj = np.loadtxt(open('data/g_tags_adj.csv', 'r'), delimiter=',', skiprows=0)\n",
    "channel_to_index = json.load(open('data/channel_indices.json', 'r'))\n",
    "g_text = from_numpy_array(g_text_adj)\n",
    "g_tags = from_numpy_array(g_tags_adj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C1: Does YouTube have a content diversity problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For each graph, calculate its diameter (i.e., the largest shortest-path length, where the maximization is done over all node pairs). What difference do you see? _Hint_: Don't worry if you get an error, just read the error message carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What does the diameter of $G_{text}$ say about the diversity of the channels’ contents? How about the diameter of $G_{tags}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Based on what you have calculated, which one has greater diversity: descriptions used by channels, or tags used by channels? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Imagine that you want to **compare** content diversity between two sets of channels (i.e., you want to see which set of channels has more diverse content), and you have calculated a tag-based graph for each set. Do you think the diameter is a good measure for doing the comparison? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Back to our own two graphs. Based on $G_{text}$, for each category of channels, which channel is the one most representative of the contents of all channels in that category? In other words, for each category, if you needed to provide a summary of all channels in the category via one channel, which channel would you choose? Show us (us being the exam designers and your friend) the descriptions of this channel’s two most-viewed videos. What metric did you use for this purpose? Explain your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C2: Going back to categories again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We want to use the two graphs to cluster channels from the same category together, and we want to compare their effectiveness at doing so. Use Kernighan-Lin bisection in the networkx package (`networkx.algorithms.community.kernighan_lin_bisection`) to divide each graph into two communities. Use 42 as the random seed. For each graph, show how many members of each category fall into each of the two communities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. If one of these graphs were ideal for this clustering task, what would the resulting communities look like? If it were the absolute worst possible graph for the task, what would the resulting communities look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Calculate the probability $P(community|category)$ for each community and category within each graph. Design a metric, using the four $P(community|category)$ values in a graph, whose value would be 1 for the ideal graph and 0 for the worst graph. Calculate this metric for both graphs and compare the two. What do the results say about how representative tags and descriptions are regarding the channel categories? Are tags better suited, or descriptions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. The Kernighan-Lin bisection you used above performs a min-edge cut: It attempts to partition the nodes of the graph into two sets of almost-equal size by deleting as few edges as possible. It starts off by creating a random partition of the nodes of the graph into two sets A and B that are almost equal in size, and then iteratively and in a greedy fashion moves nodes between A and B to reduce the number of edges between A and B. Show at least one toy example of a graph where the initialization could also be the final result. (Hint: Think of how, as we explained, the bisection algorithm relies on a minimum edge cut with a random initialization; under what circumstances could the original A and B be the best partition given that graph?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beginning of solution for C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx\n",
    "g_text_adj = np.loadtxt(open('data/g_text_adj.csv', 'r'), delimiter=',', skiprows=0)\n",
    "g_tags_adj = np.loadtxt(open('data/g_tags_adj.csv', 'r'), delimiter=',', skiprows=0)\n",
    "channel_to_index = json.load(open('data/channel_indices.json', 'r'))\n",
    "g_text = networkx.from_numpy_array(g_text_adj)\n",
    "g_tags = networkx.from_numpy_array(g_tags_adj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "networkx.diameter(g_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "networkx.diameter(g_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$G_{text}$ has a diameter of 2, while $G_{tags}$ is unconnected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C1.2\n",
    "What the diameter says in case of $G_{text}$ is that thethe longest shortest path is just 2 edges long, which could mean relatively low diversity, while $G_{tags}$ doesn't tell us much because its diameter is undefined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C1.3\n",
    "The tags are more diverse because the graph is unconnected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "networkx.number_connected_components(g_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "networkx.number_connected_components(g_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C1.4\n",
    "No, because as we saw, the diameter can end up being undefined for both, which implies that both are diverse but does not provide much of a comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C1.5\n",
    "They can use either betweenness centrality (number of shortest paths passing through the node, i.e. the channel) or just the degree. Here we use the former."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centralities = networkx.betweenness_centrality(g_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_centrality_node = sorted(centralities.keys(), key=lambda x: centralities[x], reverse=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_centrality_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{v:k for k,v in channel_to_index.items()}[highest_centrality_node]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube.loc[youtube.channel == 'Desi Perkins'].sort_values('view_count', ascending=False).head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats_df = youtube.loc[(youtube.channel_cat == 'Gaming') | (youtube.channel_cat == 'Howto & Style'), \n",
    "                   ['channel', 'channel_cat']].drop_duplicates(subset='channel')\n",
    "cats_dict = {channel_to_index[cats_df.channel.values[i]]: cats_df.channel_cat.values[i] \n",
    "             for i in range(cats_df.shape[0])}\n",
    "cats_dict = {k:1 if v == 'Gaming' else 0 for k,v in cats_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.algorithms.community import kernighan_lin_bisection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_community_probabilities(bisection, category, channel_cats):\n",
    "    total_cat = len([x for x in channel_cats if channel_cats[x] == category])\n",
    "    p_com1_cat = len([x for x in bisection[0] if channel_cats[x] == category]) / total_cat\n",
    "    p_com2_cat = len([x for x in bisection[1] if channel_cats[x] == category]) / total_cat\n",
    "    return p_com1_cat, p_com2_cat, p_com1_cat*total_cat, p_com2_cat*total_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_graph_bisection = kernighan_lin_bisection(g_text, max_iter=100, seed=42)\n",
    "tag_graph_bisection = kernighan_lin_bisection(g_tags, max_iter=100, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#networkx.connected_components(g_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For $G_{text}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percentages and numbers (% in first community, % in second community, # in first, # in second):\n",
    "\n",
    "**Gaming:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_community_probabilities(text_graph_bisection, 1, cats_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Howto & Style**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_community_probabilities(text_graph_bisection, 0, cats_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For $G_{tags}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percentages and numbers (% in first community, % in second community, # in first, # in second):\n",
    "\n",
    "**Gaming:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "calculate_community_probabilities(tag_graph_bisection, 1, cats_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Howto & Style**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_community_probabilities(tag_graph_bisection, 0, cats_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C2.2\n",
    "The ideal communities would be such that each would contain only **one** category. The worst communities would be evenly split between the two categories (i.e. 50% gaming and 50% howto)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C2.3\n",
    "Check the results in C2.1, the percentages were provided as well.\n",
    "\n",
    "For the metric, one could be:\n",
    "$$1 - 2*min_{community, category}\\{P(community|category)\\}$$\n",
    "\n",
    "which would yield 1 if one of the probabilities is 0, and yield 0 if they are all 0.5.\n",
    "\n",
    "For the two graphs we have, they can simply spot the minimum and calculate this metric, which would yield the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G_text\n",
    "1 - 2*.3125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G_tags\n",
    "1 - 2*.1458"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this, $G_{tags}$ is quite superior in its discrimination between the two categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A case where the initialisation would also be the final result would be when we have a graph with two connected components, and where each community in the initialisation is one of the two connected components. Since this is a bipartition with no edge deletions, it essentially is a local minimum of the algorithm and is also the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
