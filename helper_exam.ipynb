{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350d49ca-e67d-414f-aacd-0593a8c19a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import gzip\n",
    "import operator\n",
    "from urllib.parse import unquote\n",
    "\n",
    "import scipy.stats as stats\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "from IPython.display import Image\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from sklearn.datasets import make_blobs, make_moons, make_gaussian_quantiles\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import silhouette_score, classification_report, confusion_matrix, roc_auc_score, average_precision_score, balanced_accuracy_score,  mean_squared_error, mean_absolute_error, auc, roc_curve\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV,  cross_val_predict, cross_val_score\n",
    "from sklearn.linear_model import Ridge, LinearRegression, LogisticRegressionCV, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "pd.options.mode.chained_assignment = None "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f0c425-dcdc-463c-bd6e-59750a1e315f",
   "metadata": {},
   "source": [
    "### Tutorials\n",
    "#### Tutorial 01 - Handling data (Introduction to Pandas)\n",
    "plot, index, sort, loc, merge, concat, reshape, dummies, categories, groupby, apply\n",
    "\n",
    "#### Tutorial 02 - Data Viz and Data from the Web\n",
    "all the visualizations tools with dataframe (plot, subplots, heatmap, ...)\n",
    "\n",
    "#### Tutorial 03 - Describing data\n",
    "statistics, test significance, multiple example to plot regressions also \n",
    "\n",
    "#### Tutorial 04 - Regression analysis\n",
    "regression with the operands (+, ~, ...), OLS, logistic regression, odds, log model\n",
    "\n",
    "#### Tutorial 05 - Observational studies\n",
    "Similarity, propensity_score \n",
    "\n",
    "#### Tutorial 06 - Supervised Learning\n",
    "OLS, K-NN, LogisticRegression, Random Forest\n",
    "\n",
    "#### Tutorial 07 - Applied Machine Learning\n",
    "split train/test, create categories, confusion matrix, compute all possible scores and plot for them\n",
    "\n",
    "#### Tutorial 08 - Unsupervised Learning\n",
    "K-NN (vary k), silhouette score,elbow method, PCA, t-SNE, DBSCAN \n",
    "\n",
    "#### Tutorial 09 - Handing text\n",
    "ça skip à l'exa ce subject\n",
    "\n",
    "\n",
    "#### Tutorial 10 - Handling Networks\n",
    "Graphs \n",
    "\n",
    "#### Tutorial 11 - Scaling Up \n",
    "pyspark \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80bac98-12dc-44a5-be29-0b82187d4baf",
   "metadata": {},
   "source": [
    "**Import Files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432c0d56-dbc4-4747-8fa3-64e024529ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdf3bbd-a651-4f98-897b-e86729a0d6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(\"data/article_df_task-A.tsv.gz\", # path\n",
    "                 compression=\"infer\",  # handles compression\n",
    "                 sep=\"\\t\",             # handles .tsv\n",
    "                 error_bad_lines=False # handles bad lines\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f3619a-8fb6-4184-bcec-809e84c7fe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_data_path = './Data/twitter_data.txt'\n",
    "tweets_data = []\n",
    "with open(tweets_data_path, \"r\") as tweets_file:\n",
    "    for line in tweets_file:\n",
    "        try:\n",
    "            tweet = json.loads(line)\n",
    "            tweets_data.append(tweet)\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800b115f-1138-40fc-935f-dd2130fb159f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./files/info.txt', 'r') as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cbc7ab-a9f7-4044-b109-7f1f422dd90c",
   "metadata": {},
   "source": [
    "**Print**\n",
    "\n",
    "Use %d for decimals/integers, %f for floats (alternatively %.xf to specify a precision of x), and %s for strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba2c10f-5b99-40c8-a3bc-ae87aab9f1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are %d apples on the table.\" % (num_off_apples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57238521-c9d0-45e0-a613-5fcf01f77c91",
   "metadata": {},
   "source": [
    "**GroupBy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5737fa11-787c-499b-b9a0-e341f0668c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.groupby?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e9ac18-a493-413b-ba0b-6be1fde42ed5",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Statistics**\n",
    "\n",
    "p_value < 0.05 -> we can reject the null hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed106bd-12a8-447c-bed6-3a87b54725cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation\n",
    "stats.spearmanr?\n",
    "\n",
    "# Plot regression\n",
    "sn.lmplot?\n",
    "\n",
    "# Test if normal dist but can test other dist\n",
    "diagnostic.kstest_normal? \n",
    "\n",
    "# T-test for check if two mean same\n",
    "stats.ttest_ind?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8f0b4f-e665-47fa-bff9-273cb37d35d7",
   "metadata": {},
   "source": [
    "##### CF with bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b0a614-88b1-49ad-b4c8-530e3d56a46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_bootstrap(data, n=1000):\n",
    "    sample_statistic = [] \n",
    "    for _ in range(n):\n",
    "        sampled_data = np.random.choice(data, size=len(data))  \n",
    "        sample_statistic.append(np.mean(sampled_data))\n",
    "    return (np.percentile(sample_statistic, 2.5), np.percentile(sample_statistic, 97.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1576e22-02ac-4e72-8f02-27160c85814a",
   "metadata": {},
   "source": [
    "**Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3485157c-c4a3-46a9-9bfa-9222cc108f6a",
   "metadata": {},
   "source": [
    "- Equations are specified using patsy formula syntax. Important operators are:\n",
    "    1. `~` : Separates the left-hand side and right-hand side of a formula.\n",
    "    2. `+` : Creates a union of terms that are included in the model.\n",
    "    3. `:` : Interaction term.\n",
    "    3. `*` : `a * b` is short-hand for `a + b + a:b`, and is useful for the common case of wanting to include all interactions between a set of variables.\n",
    "    \n",
    "    \n",
    "- Intercepts are added by default.\n",
    "\n",
    "\n",
    "- Categorical variables can be included directly by adding a term C(a)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a40ad0-8aa2-4c3f-8406-372b262c1d4b",
   "metadata": {},
   "source": [
    "##### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05b5a99-d52c-41b1-9238-907d08941eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declares the model\n",
    "mod = smf.ols(formula='time ~ C(diabetes) + C(high_blood_pressure)', data=df)\n",
    "# Fits the model (find the optimal coefficients, adding a random seed ensures consistency)\n",
    "np.random.seed(2)\n",
    "res = mod.fit()\n",
    "# Print thes summary output provided by the library.\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cd5251-2878-43c4-93c6-a633e81a00d6",
   "metadata": {},
   "source": [
    "##### Logistic regression (binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a403381-0407-4114-a51d-31d35963dcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to standardize the countinuous variables\n",
    "df['age'] = (df['age'] - df['age'].mean())/df['age'].std()\n",
    "# Logit is logistic regression.\n",
    "mod = smf.logit(formula='DEATH_EVENT ~  age + creatinine_phosphokinase + ejection_fraction + \\\n",
    "                        platelets + serum_creatinine + serum_sodium + \\\n",
    "                        C(diabetes) + C(high_blood_pressure) +\\\n",
    "                        C(sex) + C(anaemia) + C(smoking) + C(high_blood_pressure)', data=df)\n",
    "res = mod.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a933be-7ad4-454c-9d71-f6fcb6991fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature names\n",
    "variables = res.params.index\n",
    "# coefficients\n",
    "coefficients = res.params.values\n",
    "# p-values\n",
    "p_values = res.pvalues\n",
    "# standard errors\n",
    "standard_errors = res.bse.values\n",
    "#confidence intervals\n",
    "res.conf_int()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e14677-58d6-4219-8577-48603231fdc7",
   "metadata": {},
   "source": [
    "**Unsupervised Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df00bc02-24df-44cf-a0c6-062df13336e2",
   "metadata": {},
   "source": [
    "##### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec92fd6-56a8-41d2-b38c-be9a6404dd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()  # create the model\n",
    "lin_reg.fit(X, y)  # train it\n",
    "\n",
    "# see the formula of the regression with the coefficient found\n",
    "for f in range(len(feature_cols)):\n",
    "    print(\"{0} * {1} + \".format(lin_reg.coef_[f], feature_cols[f]))\n",
    "print(lin_reg.intercept_)\n",
    "\n",
    "# cross_val_predict returns an array of the same size as `y` where each entry is a prediction obtained by cross validation:\n",
    "predicted = cross_val_predict(lr, X, y, cv=5)\n",
    "\n",
    "#MSE\n",
    "mean_squared_error(y, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3de505a-ab1b-4a92-a8e7-808adcccd6f0",
   "metadata": {},
   "source": [
    "##### Ridge Regression\n",
    "\n",
    "puts a penalty on large weights Beta and forces them to be smaller in magnitude. This reduces the complexity of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1106012f-295a-480a-b165-d7f3058b00e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge(alpha=6)\n",
    "predicted_r = cross_val_predict(ridge, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5c0fd3-77c8-44f0-8d1a-706f85a55138",
   "metadata": {},
   "source": [
    "##### LogisticRegresion\n",
    "\n",
    "Logistic regression uses a threshold on the probability to decide at which class to assign a prediction. In some cases, we are interested to understand how the model behaves at different levels of this threshold (ROC curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab5afa0-0a91-4b33-8df4-f501ced5b10c",
   "metadata": {},
   "source": [
    "Feature Vectors (dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba8b91d-a29a-4de4-bc4f-3c0b4da28e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The features vector\n",
    "X = pd.get_dummies(titanic[titanic_features])\n",
    "\n",
    "# Can fill empty rows/cells\n",
    "X = X.fillna(X.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4ab30c-a743-444d-a6a0-11d0f2c165ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "# Possible way to see utput for one test\n",
    "\"YES\" if logistic.predict([test])[0] > 0 else \"NO\"\n",
    "# Probability decision for a test \n",
    "logistic.predict_proba([test])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92c7506-42b9-4ad5-a02c-161babd9ca23",
   "metadata": {},
   "source": [
    "Can change a \"continous\" vector to a \"discrete\" vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303a3abf-a42f-4bfb-9319-53afbc2cd4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = y_train.median()\n",
    "\n",
    "y_train_binary = (y_train > threshold).astype(int)\n",
    "y_test_binary = (y_test > threshold).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d0dcbd-f6d6-47e5-b860-9068f964b466",
   "metadata": {},
   "source": [
    "###### Logisitic Regression Topic classification (multi-class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06e7883-926f-44bd-ab79-1886ac8ada5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "parameters = {\n",
    "    'clf__alpha': [1e-4],    \n",
    "}\n",
    "\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier(penalty='l2', loss='log', max_iter=5, tol=None, random_state=42))\n",
    "])\n",
    "\n",
    "\n",
    "gs_clf = GridSearchCV(text_clf, parameters, cv=5)\n",
    "gs_clf = gs_clf.fit(X_train, y_train)\n",
    "\n",
    "print(gs_clf.best_score_)\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))\n",
    "    \n",
    "predicted = gs_clf.predict(X_test)\n",
    "print(\"Accuracy on Test Data: \", np.mean(predicted == y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83b3291-e93f-47ba-a4e3-e9d8298912a6",
   "metadata": {},
   "source": [
    "##### Ridge Regression\n",
    "\n",
    "Technique for analyzing multiple regression data that suffer from multicollinearity.\n",
    "\n",
    "Find and use the optimal regularization parameter $\\alpha$ from the set {0.001, 0.01, 0.1} via 3-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17e827d-8fb7-4a44-9710-adf51b384724",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge()\n",
    "ridge_hyper = {'alpha':(0.001, 0.01, 0.1)}\n",
    "ridge_cv = GridSearchCV(ridge, ridge_hyper, cv=3)\n",
    "ridge_cv.fit(X_train, y_train)\n",
    "\n",
    "ridge_cv.cv_results_['mean_test_score']\n",
    "\n",
    "mean_absolute_error(y_test, ridge_cv.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2741c012-ca18-482c-8642-70a0de32b706",
   "metadata": {},
   "source": [
    "Precision and Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7067141-652a-49d7-9fea-b6338c45e50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = cross_val_score(logistic, X, y, cv=10, scoring=\"precision\")\n",
    "recall = cross_val_score(logistic, X, y, cv=10, scoring=\"recall\")\n",
    "\n",
    "# Precision: avoid false positives\n",
    "print(\"Precision: %0.2f (+/- %0.2f)\" % (precision.mean(), precision.std() * 2))\n",
    "# Recall: avoid false negatives\n",
    "print(\"Recall: %0.2f (+/- %0.2f)\" % (recall.mean(), recall.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b959fd82-8d3c-4a84-b518-9d283d095dd4",
   "metadata": {},
   "source": [
    "##### K-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c60048-11e9-48c5-9156-2fb7575b3f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K = 1\n",
    "clf_moons_1 = KNeighborsClassifier(1)\n",
    "clf_moons_1.fit(X_moons, y_moons)\n",
    "clf_circles_1 = KNeighborsClassifier(1)\n",
    "clf_circles_1.fit(X_circles, y_circles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5039d1-66a5-4b18-853b-f4635071900e",
   "metadata": {},
   "source": [
    "##### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7415009-31a7-4736-83ef-5c349b086001",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_list = np.linspace(0.05, 0.15, 14)\n",
    "labels = DBSCAN(eps=eps).fit_predict(X_moons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa50238e-23ba-4855-89bc-5f185ddefae1",
   "metadata": {},
   "source": [
    "##### Random Forest Model\n",
    "\n",
    "- Use random forest classifier with max tree depth of 3 (and random_state=0)\n",
    "- Train the classifier by variating the number of trees from 1 to 20 (N)\n",
    "- For each step estimate precision/recall with cross validation (10-folds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88719305-e0f9-4f9c-8e7e-cc406ca0545e",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_trees = [n for n in range(1, 21)]\n",
    "precision_scores = []\n",
    "recalls_scores = []\n",
    "\n",
    "\n",
    "for nt in number_trees:\n",
    "    clf = RandomForestClassifier(max_depth=3, random_state=0, n_estimators=nt)\n",
    "    clf.fit(X, y)\n",
    "    precision = cross_val_score(clf, X, y, cv=10, scoring=\"precision\")\n",
    "    precision_scores.append(precision.mean())\n",
    "    recall = cross_val_score(clf, X, y, cv=10, scoring=\"recall\")\n",
    "    recalls_scores.append(recall.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a37951-ccf4-41f7-aa0b-f5b16ef436bc",
   "metadata": {},
   "source": [
    "**Observational studies**\n",
    "\n",
    "**Propensity score** of a data point represents its probability of receiving the treatment, based on its pre-treatment features (in this case, age, education, pre-treatment income, etc.)\n",
    "\n",
    "**Observational studies** are ones where researchers observe the effect of a risk factor, diagnostic test, treatment or other intervention without trying to change who is or isn't exposed to it. Cohort studies and case control studies are two types of observational studies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd80c14-6715-4a49-b470-57d8d65ccd42",
   "metadata": {},
   "source": [
    "##### Balancing the dataset via matching (Graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141ab65c-4253-44f6-8361-f9f92e992803",
   "metadata": {},
   "source": [
    "$$ similarity(x,y) = 1 - | propensity\\_score(x) - propensity\\_score(y) |$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b9ac99-dc43-4dec-96f7-cf010077b582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity(propensity_score1, propensity_score2):\n",
    "    '''Calculate similarity for instances with given propensity scores'''\n",
    "    return 1-np.abs(propensity_score1-propensity_score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d066508a-a8d3-4cc7-85d5-e8dab373b13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the treatment and control groups\n",
    "treatment_df = lalonde_data[lalonde_data['treat'] == 1]\n",
    "control_df = lalonde_data[lalonde_data['treat'] == 0]\n",
    "\n",
    "# Create an empty undirected graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Loop through all the pairs of instances\n",
    "for control_id, control_row in control_df.iterrows():\n",
    "    for treatment_id, treatment_row in treatment_df.iterrows():\n",
    "\n",
    "        # Calculate the similarity \n",
    "        similarity = get_similarity(control_row['Propensity_score'],\n",
    "                                    treatment_row['Propensity_score'])\n",
    "\n",
    "        # Add an edge between the two instances weighted by the similarity between them\n",
    "        G.add_weighted_edges_from([(control_id, treatment_id, similarity)])\n",
    "\n",
    "# Generate and return the maximum weight matching on the generated graph\n",
    "matching = nx.max_weight_matching(G)\n",
    "\n",
    "matched = [i[0] for i in list(matching)] + [i[1] for i in list(matching)]\n",
    "\n",
    "# new datafram with only the matched pairs\n",
    "balanced_df_1 = lalonde_data.iloc[matched]\n",
    "\n",
    "#can add conditions to have a better match if for example a feature is not balanced (in this tutorial, it's race)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453edcf9-5006-4254-9953-f9dd80d8bbb5",
   "metadata": {},
   "source": [
    "**Processing Dataset before ML** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dddc96-e926-4fb3-8fee-d5092ba3cea5",
   "metadata": {},
   "source": [
    "##### Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd04f86-6962-4dbc-afaf-d33e81526e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_set(data_to_split, ratio=0.8):\n",
    "    mask = np.random.rand(len(data_to_split)) < ratio\n",
    "    return [data_to_split[mask].reset_index(drop=True), data_to_split[~mask].reset_index(drop=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81b07b7-b6cb-4d28-839f-72a4d62df82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without X,y ... \"only\" X\n",
    "train, test = train_test_split(youtube_ml, test_size=0.3, random_state=42)\n",
    "\n",
    "# With X,y \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e304954-d976-4964-9bca-735933fca41d",
   "metadata": {},
   "source": [
    "##### Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48cccc2-a2f6-44ab-87e5-ca835e95b5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = ['sex_upon_outcome', 'animal_type', 'intake_condition',\n",
    "                       'intake_type', 'sex_upon_intake']\n",
    "train_categorical = pd.get_dummies(train, columns=categorical_columns)\n",
    "train_categorical.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8506897a-fb42-4cc0-b1cd-bb60c34bc420",
   "metadata": {},
   "source": [
    "##### Label/Features (y, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e7e2a2-1ed2-43c4-85f6-2730de557a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label=train_categorical.adopted\n",
    "train_features = train_categorical.drop('adopted', axis=1)\n",
    "\n",
    "test_label=test_categorical.adopted\n",
    "test_features = test_categorical.drop('adopted', axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8ba175-03c2-4eb9-89ca-1200ca53f99e",
   "metadata": {},
   "source": [
    "##### Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e266c934-86e1-4979-9548-f9f7819cf043",
   "metadata": {},
   "outputs": [],
   "source": [
    "means = train_features.mean()\n",
    "stddevs = train_features.std()\n",
    "\n",
    "train_features_std = pd.DataFrame()\n",
    "for c in train_features.columns:\n",
    "    train_features_std[c] = (train_features[c]-means[c])/stddevs[c]\n",
    "\n",
    "# Use the mean and stddev of the training set\n",
    "test_features_std = pd.DataFrame()\n",
    "for c in test_features.columns:\n",
    "    test_features_std[c] = (test_features[c]-means[c])/stddevs[c]\n",
    "\n",
    "train_features_std.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656f4ca5-b599-42a7-bd5b-2c997aa44c4d",
   "metadata": {},
   "source": [
    "**Score ML**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8462bf4-3229-4b13-9a01-9ea371167ed7",
   "metadata": {},
   "source": [
    "##### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e5b845-906c-4917-99c9-2bdc1fc139fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confusion_matrix(true_label, prediction_proba, decision_threshold=0.5): \n",
    "    \n",
    "    predict_label = (prediction_proba[:,1]>decision_threshold).astype(int)   \n",
    "                                                                                                                       \n",
    "    TP = np.sum(np.logical_and(predict_label==1, true_label==1))\n",
    "    TN = np.sum(np.logical_and(predict_label==0, true_label==0))\n",
    "    FP = np.sum(np.logical_and(predict_label==1, true_label==0))\n",
    "    FN = np.sum(np.logical_and(predict_label==0, true_label==1))\n",
    "    \n",
    "    confusion_matrix = np.asarray([[TP, FP],\n",
    "                                    [FN, TN]])\n",
    "    return confusion_matrix\n",
    "\n",
    "\n",
    "def compute_all_score(confusion_matrix, t=0.5):\n",
    "    [[TP, FP],[FN, TN]] = confusion_matrix.astype(float)\n",
    "    \n",
    "    accuracy =  (TP+TN)/np.sum(confusion_matrix)\n",
    "    \n",
    "    precision_positive = TP/(TP+FP) if (TP+FP) !=0 else np.nan\n",
    "    precision_negative = TN/(TN+FN) if (TN+FN) !=0 else np.nan\n",
    "    \n",
    "    recall_positive = TP/(TP+FN) if (TP+FN) !=0 else np.nan\n",
    "    recall_negative = TN/(TN+FP) if (TN+FP) !=0 else np.nan\n",
    "\n",
    "    F1_score_positive = 2 *(precision_positive*recall_positive)/(precision_positive+recall_positive) if (precision_positive+recall_positive) !=0 else np.nan\n",
    "    F1_score_negative = 2 *(precision_negative*recall_negative)/(precision_negative+recall_negative) if (precision_negative+recall_negative) !=0 else np.nan\n",
    "\n",
    "    return [t, accuracy, precision_positive, recall_positive, F1_score_positive, precision_negative, recall_negative, F1_score_negative]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c763754c-4a34-4245-ac9c-ecd635435f49",
   "metadata": {},
   "source": [
    "##### Silhouette score \n",
    "- k vs silhouette score plot\n",
    "- need to take the highest score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798a9845-51ba-4f06-8ef5-ffaac190109d",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouettes = []\n",
    "\n",
    "# Try multiple k\n",
    "for k in range(2, 11):\n",
    "    # Cluster the data and assigne the labels\n",
    "    labels = KMeans(n_clusters=k, random_state=10).fit_predict(X)\n",
    "    # Get the Silhouette score\n",
    "    score = silhouette_score(X, labels)\n",
    "    silhouettes.append({\"k\": k, \"score\": score})\n",
    "    \n",
    "# Convert to dataframe\n",
    "silhouettes = pd.DataFrame(silhouettes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92359c1a-c0a9-471c-818f-e7762b15b612",
   "metadata": {},
   "source": [
    "##### Elbow method\n",
    "- sum of squared errors vs k\n",
    "- quand ça break sur le plot wesh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8383afa2-8609-4af7-8a54-cada2e98c72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sse(features_X, start=2, end=11):\n",
    "    sse = []\n",
    "    for k in range(start, end):\n",
    "        # Assign the labels to the clusters\n",
    "        kmeans = KMeans(n_clusters=k, random_state=10).fit(features_X)\n",
    "        sse.append({\"k\": k, \"sse\": kmeans.inertia_})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39475036-92e3-464a-b585-1505df383314",
   "metadata": {},
   "source": [
    "**High Dimensional Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdc32fb-f657-48f4-b61a-0f4f74993690",
   "metadata": {},
   "source": [
    "Dimensionality Reduction reduce the number of dimensions by preserving as much information as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679bac4a-3b7b-4142-a5d2-c532791f6f7e",
   "metadata": {},
   "source": [
    "##### t-SNE\n",
    "- It is a non-linear Dimensionality reduction technique\n",
    "-  It embeds the points from a higher dimension to a lower dimension trying to preserve the neighborhood of that point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7e45eb-4e08-4fbd-b0cc-9da6dc5320e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced_tsne = TSNE(n_components=2, random_state=0).fit_transform(X10d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e5ef48-ad70-449a-a587-7be928d009ff",
   "metadata": {},
   "source": [
    "##### PCA \n",
    "- It is a linear Dimensionality reduction technique\n",
    "- The main idea behind this technique is to reduce the dimensionality of data that is highly correlated by transforming the original set of vectors to a new set which is known as Principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b77c4e-cfc2-41a3-b18b-befd4af23650",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced_pca = PCA(n_components=2).fit(X10d).transform(X10d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9ab235-fe91-4c04-9504-ef8a08341581",
   "metadata": {},
   "source": [
    "**Graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2425e58-7935-41a3-b164-3cdf72f2c616",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph() # for a directed graph use nx.DiGraph()\n",
    "nx.info(G)\n",
    "\n",
    "# add node attributes by passing dictionary of type name -> attribute\n",
    "nx.set_node_attributes(quakerG, nodes['Role'].to_dict(), 'Role' )\n",
    "\n",
    "#Create graph from numpy array\n",
    "g_text = networkx.from_numpy_array(g_text_adj)\n",
    "\n",
    "#Find diameter of a graph (largest shortest-path length, where the maximization is done over all node pairs)\n",
    "networkx.diameter(g_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c96dde-bc18-4025-a826-9166bf5d5801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e50d764-3565-4457-a3a9-f528e4de99e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can easily get the attributes of a node\n",
    "quakerG.nodes['William Penn']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a114063d-0742-43cf-83ae-bec585f4dcc0",
   "metadata": {},
   "source": [
    "##### Degree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0879828-7420-48c5-8dcc-90418a573296",
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = dict(quakerG.degree(quakerG.nodes()))\n",
    "sorted_degree = sorted(degrees.items(), key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed551f5-a0c7-493e-9a31-257801edc044",
   "metadata": {},
   "source": [
    "##### Betweeness Centrality\n",
    "\n",
    "the more shortest paths pass through a node, the more important it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc600dc8-7f4d-4902-b59f-8c39f0c2db6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute betweenness centrality\n",
    "betweenness = nx.betweenness_centrality(quakerG)\n",
    "# Assign the computed centrality values as a node-attribute in your network\n",
    "nx.set_node_attributes(quakerG, betweenness, 'betweenness')\n",
    "sorted_betweenness = sorted(betweenness.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "for quaker, bw in sorted_betweenness[:5]:\n",
    "    print(quaker, 'who is', quakerG.nodes[quaker]['Role'], 'has betweeness: %.3f' %bw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1f2b11-5221-4693-b5d1-9b6b6b622dde",
   "metadata": {},
   "source": [
    "##### Sparsity\n",
    "\n",
    "$L = \\frac{|E|}{|E_{max}|}$, where $E_{max} = \\frac{n * (n-1)}{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d4621e-07a9-482f-b3a7-3d2e0690af5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.density(quakerG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37607326-a8c3-4b8e-b484-8966a8ab1b40",
   "metadata": {},
   "source": [
    "##### Connected Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b404d4-399a-4b6c-bad1-7f4c1056d6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the number of connected components\n",
    "networkx.number_connected_components(g_text)\n",
    "\n",
    "# WC if the underlying undirected graph obtained by replacing all directed edges of the graph with undirected edges is a connected graph \n",
    "nx.is_weakly_connected(G)\n",
    "nx.weakly_connected_components(G)\n",
    "\n",
    "# SC if, for every pair of vertices $(u, v)$, it contains a directed path from $u$ to $v$ and a directed path from $v$ to $u$.\n",
    "nx.is_strongly_connected(G)\n",
    "nx.strongly_connected_components(G)\n",
    "\n",
    "largest_cc = max(nx.weakly_connected_components(G), key=len)\n",
    "H = G.subgraph(list(largest_cc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a93bff-412c-497a-94a3-558a2fc1721b",
   "metadata": {},
   "source": [
    "##### Diameter and Shortest Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8315a8-d5d6-4541-aaa5-46e4d679aa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "fell_whitehead_path = nx.shortest_path(quakerG, source=\"Margaret Fell\", target=\"George Whitehead\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699d675f-1330-42a4-b902-e2dca1c4d49d",
   "metadata": {},
   "source": [
    "##### Transitivity\n",
    "\n",
    "Global clustering coefficient, or the ratio of all existing triangles (closed triples) over all possible triangles (open and closed triplets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47e418f-bb31-426c-8fc0-f37ae4d9e7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.transitivity(quakerG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1b272f-97d1-48de-93c6-79844bd85817",
   "metadata": {},
   "source": [
    "##### Louvain Method\n",
    "\n",
    "Community detection is a method to extract communities from large networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7a667f-0897-4a86-9db1-c4cef904f0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = community_louvain.best_partition(quakerG)\n",
    "# add it as an attribute to the nodes\n",
    "for n in quakerG.nodes:\n",
    "    quakerG.nodes[n][\"louvain\"] = partition[n]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d819155b-aaab-4382-8321-98851c894676",
   "metadata": {},
   "source": [
    "**Spark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36b22c4-0c62-49fa-9feb-92794bf53033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# create the context\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805a496e-7905-4a4f-b9a0-56ea89160024",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bombing_Operations = spark.read.json(\"Bombing_Operations.json\")\n",
    "\n",
    "Bombing_Operations.printSchema()\n",
    "\n",
    "Bombing_Operations.take(3)\n",
    "\n",
    "#move to pandas\n",
    "missions_count_pd = missions_counts.toPandas()\n",
    "missions_count_pd.head()\n",
    "\n",
    "# select column\n",
    "missions_aircrafts = missions_joined.select(\"AirCraftType\")\n",
    "\n",
    "#sort by a specific column\n",
    "by_posts = subreddit_info.select(\"subreddit\", \"total_posts\").sort(col(\"total_posts\").desc())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d5fe82-8d1e-4ed6-8c9d-899ef9ad7213",
   "metadata": {},
   "source": [
    "##### GroupBy (SQL or not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b4b7a5-5b0e-4f0a-8555-a3b6478e8efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "missions_counts = Bombing_Operations.groupBy(\"ContryFlyingMission\")\\\n",
    "                                    .agg(count(\"*\").alias(\"MissionsCount\"))\\\n",
    "                                    .sort(desc(\"MissionsCount\"))\n",
    "missions_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd839879-60a7-4651-a8b8-26fa68cdeed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bombing_Operations.registerTempTable(\"Bombing_Operations\")\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT ContryFlyingMission, count(*) as MissionsCount\n",
    "FROM Bombing_Operations\n",
    "GROUP BY ContryFlyingMission\n",
    "ORDER BY MissionsCount DESC\n",
    "\"\"\"\n",
    "\n",
    "missions_counts = spark.sql(query)\n",
    "missions_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d549966b-efd4-4423-9ec3-203479ae1c81",
   "metadata": {},
   "source": [
    "##### Jaccard Similarity\n",
    "\n",
    "$Jaccard(A,B) = \\frac{|A \\cap B|}{|A \\cup B|}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ec52d3-f129-4940-a98c-2d2e0bf7ee39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(list1, list2):\n",
    "    s1 = set(list1)\n",
    "    s2 = set(list2)\n",
    "    return len(s1.intersection(s2)) / len(s1.union(s2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234911ca-abc4-4658-a104-0e8fe1d10879",
   "metadata": {},
   "source": [
    "##### Processing text with Pyspark\n",
    "\n",
    "More on the tutorial's exercice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571fb20d-4cb9-4288-884a-b71a7e9672ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the text\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"body\", outputCol=\"all_words\", pattern=\"\\\\W\")\n",
    "reddit_with_words = regexTokenizer.transform(reddit)\n",
    "\n",
    "# remove stop words\n",
    "remover = StopWordsRemover(inputCol=\"all_words\", outputCol=\"words\")\n",
    "reddit_with_tokens = remover.transform(reddit_with_words).drop(\"all_words\")\n",
    "\n",
    "# get all words in a single dataframe\n",
    "all_words = reddit_with_tokens.select(explode(\"words\").alias(\"word\"))\n",
    "# group by, sort and limit to 50k \n",
    "top50k = all_words.groupBy(\"word\").agg(count(\"*\").alias(\"total\")).sort(col(\"total\").desc()).limit(50000)\n",
    "\n",
    "top50k.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
